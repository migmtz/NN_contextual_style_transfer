{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>PROJECT SANDBOX</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "The aim of this notebook is to provide a simple sandbox to test different NN architectures for the project. , here is a doc about the functions imported from `scripts` folder : \n",
    "\n",
    "- **`prepare_dataset(device,ratio=0.5,shuffle_ctx=False)`** :\n",
    "    - **Input**:\n",
    "        - device : a torch.device object\n",
    "        - ratio : a float ratio between 0 and 1 that determines the average proportion of modern english verses in the data loader\n",
    "        - shuffle_ctx : if `True`, shuffle the contexts within a Batch so that half of the `x_1` elements has a wrong context `ctx_1`. Useful to train the context recognizer model.\n",
    "    - **Return** :\n",
    "        - a torch Dataset | class : Shakespeare inherited from torch.utils.data.Dataset\n",
    "        - a python word dictionary (aka tokenizer) | class : dict\n",
    "    - **Tensors returned when loaded in the dataloader**:\n",
    "        - x_1 : input verse (modern / shakespearian)\n",
    "        - x_2 : output verse (modern / shakespearian)\n",
    "\n",
    "        - ctx_1 = context of the input verse\n",
    "        - ctx_2 = context of the output verse\n",
    "\n",
    "        - len_x : length of the input verse\n",
    "        - len_y : length of the output verse\n",
    "\n",
    "        - len_ctx_x : length of the input verse context\n",
    "        - len_ctx_y : length of the output verse context\n",
    "\n",
    "        - label : label of the input verse (0 : modern, 1 : shakespearian)\n",
    "        - label_ctx : label of the context (0 : wrong context, 1 : right context)\n",
    "- **`string2code(string,dict)`** : \n",
    "    - **Input**:\n",
    "        - string : a sentence\n",
    "        - dict : a tokenizer\n",
    "    - **Return** :\n",
    "        - a torch Longtensor (sentence tokenized)\n",
    "- **`code2string(torch.Longtensor,dict)`** : \n",
    "    - **Input**:\n",
    "        - torch.Longtensor : a sentence tokenized\n",
    "        - dict : a tokenizer\n",
    "    - **Return** :\n",
    "        - a string sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "from scripts.data_builders.prepare_dataset import prepare_dataset,string2code,code2string,assemble\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pickle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device = \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ...\n",
      "- Shakespeare dataset length :  21079\n",
      "- Corrupted samples (ignored) :  0\n"
     ]
    }
   ],
   "source": [
    "train_data, dict_words = prepare_dataset(device,ratio=0.5,shuffle_ctx=True) #check with shift+tab to look at the data structure\n",
    "batch_size = 128\n",
    "dict_token = {b:a for a,b in dict_words.items()} #dict for code2string\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                           shuffle=True,collate_fn=train_data.collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(dict_words) #19089\n",
    "d_embedding = 300 #cf. paper Y.Kim 2014 Convolutional Neural Networks for Sentence Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoherenceClassifier(torch.nn.Module):\n",
    "    def __init__(self,dict_size=dict_size,d_embedding=300):\n",
    "        super().__init__()\n",
    "        self.embed_layer=torch.nn.Embedding(dict_size+1,d_embedding,padding_idx=dict_size)\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv1d(d_embedding,3,kernel_size = 3, stride = 1)\n",
    "        self.max_pool = torch.nn.MaxPool1d(3,2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear = torch.nn.Linear(3,1)\n",
    "        # self.f=lambda x: torch.norm(x,dim=1)**2 (I am not sure it is necessary at all)\n",
    "        self.f = lambda x:x\n",
    "        self.sigmoid=torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x,ctx):\n",
    "        x = self.embed_layer(x)\n",
    "        ctx = self.embed_layer(ctx)\n",
    "        x = torch.cat((self.f(x),ctx),dim=1)\n",
    "        x = self.conv_1( x.transpose(1,2))\n",
    "        x = self.max_pool( x )\n",
    "        x = self.relu( x )\n",
    "        x = torch.max( x , 2 )[0]\n",
    "        x = self.sigmoid(self.linear(x))\n",
    "        return(x)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoherenceClassifier(torch.nn.Module):\n",
    "    def __init__(self,dict_size=dict_size,d_embedding=300,d_hidden=100):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.embedding = nn.Embedding(dict_size+1,d_embedding,padding_idx=dict_size)\n",
    "        self.lstm = nn.LSTM(d_embedding,self.d_hidden,dropout=0.,num_layers=1,bidirectional=False)\n",
    "        self.linear = torch.nn.Linear(self.d_hidden,1)\n",
    "    \n",
    "    def forward(self,x,len_x):\n",
    "        x = self.embedding(x)\n",
    "        x = pack_padded_sequence(x.permute(1,0,2),len_x,enforce_sorted=False)\n",
    "        _,x = self.lstm(x)\n",
    "        x = x[0].reshape(-1,self.d_hidden)\n",
    "        x = torch.sigmoid( self.linear(x) ).reshape(-1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y , ctx_x,ctx_y , len_x,len_y , len_ctx_x,len_ctx_y, label,label_ctx in train_loader:\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        print(\"\\n- x :\")\n",
    "        print(code2string(x[i],dict_token))\n",
    "        print(\"- context of x :\")\n",
    "        print(code2string(ctx_x[i],dict_token))\n",
    "        print(\"- context label :\",label_ctx[i].item())\n",
    "\n",
    "        print(\"- len_ctx_x :\")\n",
    "        print(ctx_x)\n",
    "#         ipdb.set_trace()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "model=CoherenceClassifier().to(device)\n",
    "optimizer= optim.Adam(params=model.parameters(),lr=0.001)\n",
    "loss_func=BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 0.69855\n",
      "1 \t 0.69757\n",
      "2 \t 0.69741\n",
      "3 \t 0.69733\n",
      "4 \t 0.69755\n",
      "5 \t 0.69745\n",
      "6 \t 0.6974\n",
      "7 \t 0.69739\n",
      "8 \t 0.69747\n",
      "9 \t 0.69746\n",
      "10 \t 0.69745\n",
      "11 \t 0.69739\n",
      "12 \t 0.69744\n",
      "13 \t 0.69742\n",
      "14 \t 0.69731\n",
      "15 \t 0.69744\n",
      "16 \t 0.69743\n",
      "17 \t 0.69742\n",
      "18 \t 0.69741\n",
      "19 \t 0.69742\n",
      "20 \t 0.69744\n",
      "21 \t 0.69741\n",
      "22 \t 0.69741\n",
      "23 \t 0.69741\n",
      "24 \t 0.69742\n",
      "25 \t 0.69743\n",
      "26 \t 0.69738\n",
      "27 \t 0.69746\n",
      "28 \t 0.69741\n",
      "29 \t 0.69741\n",
      "30 \t 0.6974\n",
      "31 \t 0.69742\n",
      "32 \t 0.69742\n",
      "33 \t 0.6974\n",
      "34 \t 0.6974\n",
      "35 \t 0.69739\n",
      "36 \t 0.69741\n",
      "37 \t 0.69742\n",
      "38 \t 0.69738\n",
      "39 \t 0.69738\n",
      "40 \t 0.69741\n",
      "41 \t 0.69742\n",
      "42 \t 0.69741\n",
      "43 \t 0.69737\n",
      "44 \t 0.69745\n",
      "45 \t 0.6974\n",
      "46 \t 0.69738\n",
      "47 \t 0.69746\n",
      "48 \t 0.69739\n",
      "49 \t 0.69743\n",
      "50 \t 0.69742\n",
      "51 \t 0.69742\n",
      "52 \t 0.69743\n",
      "53 \t 0.69744\n",
      "54 \t 0.69743\n",
      "55 \t 0.69736\n",
      "56 \t 0.69743\n",
      "57 \t 0.69745\n",
      "58 \t 0.69744\n",
      "59 \t 0.69743\n",
      "60 \t 0.69741\n",
      "61 \t 0.69743\n",
      "62 \t 0.69742\n",
      "63 \t 0.6974\n",
      "64 \t 0.69743\n",
      "65 \t 0.69742\n",
      "66 \t 0.69745\n",
      "67 \t 0.6974\n",
      "68 \t 0.69739\n",
      "69 \t 0.69742\n",
      "70 \t 0.69734\n",
      "71 \t 0.69747\n",
      "72 \t 0.69742\n",
      "73 \t 0.69742\n",
      "74 \t 0.69734\n",
      "75 \t 0.69745\n",
      "76 \t 0.69737\n",
      "77 \t 0.69746\n",
      "78 \t 0.6974\n",
      "79 \t 0.69743\n",
      "80 \t 0.69737\n",
      "81 \t 0.69738\n",
      "82 \t 0.69741\n",
      "83 \t 0.69745\n",
      "84 \t 0.69739\n",
      "85 \t 0.69738\n",
      "86 \t 0.69738\n",
      "87 \t 0.69738\n",
      "88 \t 0.69742\n",
      "89 \t 0.69744\n",
      "90 \t 0.69742\n",
      "91 \t 0.69735\n",
      "92 \t 0.69747\n",
      "93 \t 0.69735\n",
      "94 \t 0.69741\n",
      "95 \t 0.6974\n",
      "96 \t 0.6974\n",
      "97 \t 0.69743\n",
      "98 \t 0.6974\n",
      "99 \t 0.69737\n"
     ]
    }
   ],
   "source": [
    "n = len(train_data.x) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x,y , ctx_x,ctx_y , len_x,len_y , len_ctx_x,len_ctx_y, label,label_ctx in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = model.forward(x,len_x)\n",
    "        loss = loss_func(x,label_ctx.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(epoch,\"\\t\",round(total_loss/n,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
