{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>PROJECT SANDBOX</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "The aim of this notebook is to provide a simple sandbox to test different NN architectures for the project. , here is a doc about the functions imported from `scripts` folder : \n",
    "\n",
    "- **`prepare_dataset(device,ratio=0.5,shuffle_ctx=False)`** :\n",
    "    - **Input**:\n",
    "        - device : a torch.device object\n",
    "        - ratio : a float ratio between 0 and 1 that determines the average proportion of modern english verses in the data loader\n",
    "        - shuffle_ctx : if `True`, shuffle the contexts within a Batch so that half of the `x_1` elements has a wrong context `ctx_1`. Useful to train the context recognizer model.\n",
    "    - **Return** :\n",
    "        - a torch Dataset | class : Shakespeare inherited from torch.utils.data.Dataset\n",
    "        - a python word dictionary (aka tokenizer) | class : dict\n",
    "    - **Tensors returned when loaded in the dataloader**:\n",
    "        - x_1 : input verse (modern / shakespearian)\n",
    "        - x_2 : output verse (modern / shakespearian)\n",
    "\n",
    "        - ctx_1 = context of the input verse\n",
    "        - ctx_2 = context of the output verse\n",
    "\n",
    "        - len_x : length of the input verse\n",
    "        - len_y : length of the output verse\n",
    "\n",
    "        - len_ctx_x : length of the input verse context\n",
    "        - len_ctx_y : length of the output verse context\n",
    "\n",
    "        - label : label of the input verse (0 : modern, 1 : shakespearian)\n",
    "        - label_ctx : label of the context (0 : wrong context, 1 : right context)\n",
    "- **`string2code(string,dict)`** : \n",
    "    - **Input**:\n",
    "        - string : a sentence\n",
    "        - dict : a tokenizer\n",
    "    - **Return** :\n",
    "        - a torch Longtensor (sentence tokenized)\n",
    "- **`code2string(torch.Longtensor,dict)`** : \n",
    "    - **Input**:\n",
    "        - torch.Longtensor : a sentence tokenized\n",
    "        - dict : a tokenizer\n",
    "    - **Return** :\n",
    "        - a string sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'assemble'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-738b68d33c89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_builders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstring2code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcode2string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0massemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'assemble'"
     ]
    }
   ],
   "source": [
    "from scripts.data_builders.prepare_dataset import prepare_dataset,string2code,code2string,assemble\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import ipdb\n",
    "import pickle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device = \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ...\n",
      "- Shakespeare dataset length :  20316\n",
      "- Corrupted samples (ignored) :  763\n"
     ]
    }
   ],
   "source": [
    "train_data, dict_words = prepare_dataset(device,ratio=0.5,shuffle_ctx=True) #check with shift+tab to look at the data structure\n",
    "batch_size = 20\n",
    "dict_token = {b:a for a,b in dict_words.items()} #dict for code2string\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                           shuffle=True,collate_fn=train_data.collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size=len(dict_words) #19089\n",
    "d_embedding=300 #cf. paper Y.Kim 2014 Convolutional Neural Networks for Sentence Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoherenceClassifier(torch.nn.Module):\n",
    "    def __init__(self,dict_size=dict_size,d_embedding=300,max_length=100):\n",
    "        super().__init__()\n",
    "        self.embed_layer=torch.nn.Embedding(dict_size,d_embedding)\n",
    "        self.conv_1 = torch.nn.Conv1d(d_embedding,3,kernel_size = 3, stride = 1)\n",
    "        self.max_pool = torch.nn.MaxPool1d(3,2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear = torch.nn.Linear(3,1)\n",
    "        # self.f=lambda x: torch.norm(x,dim=1)**2 (I am not sure it is necessary at all)\n",
    "        self.f=lambda x:x\n",
    "        self.tanh=torch.nn.Tanh()\n",
    "        self.softmax=torch.nn.Softmax()\n",
    "    \n",
    "    def forward(self,x,ctx):\n",
    "        input_=asse\n",
    "        x = self.conv_1(x.transpose(1,2))\n",
    "        x = self.max_pool(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.max(x,2)[0]\n",
    "        u=self.LM()\n",
    "        x = self.softmax(self.tanh(self.linear(u)))\n",
    "        return(x)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- x :\n",
      "RIDE , RIDE , MESSALA .\n",
      "- context of x :\n",
      "TRANIO , THAT FACED AND BRAVED ME IN THIS MATTER SO ? WHY , TELL ME , IS NOT THIS MY CAMBIO ? LOVE WROUGHT THESE MIRACLES . BIANCA’S LOVE MADE ME EXCHANGE MY STATE WITH TRANIO , WHILE HE DID BEAR MY COUNTENANCE IN THE TOWN , AND HAPPILY I HAVE ARRIVÈD AT THE LAST UNTO THE WISHÈD\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "HELENA , WE’LL TELL YOU ABOUT OUR SECRET PLAN .\n",
      "- context of x :\n",
      "DON’T WORRY ABOUT ME . — YOUR SISTER AND THE DUKE ARE HERE . NOBLE WORDS , SIR . THEN LET’S MEET WITH OUR SENIOR COMMAND AND DISCUSS WHAT TO DO NEXT . I’LL MEET YOU AT YOUR TENT .\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "THEY ONLY LIKE THINGS AS BAD AS THEMSELVES .\n",
      "- context of x :\n",
      "IF THE WITCHES TELL THE TRUTH — WHICH THEY DID ABOUT YOU — MAYBE WHAT THEY SAID ABOUT ME WILL COME TRUE TOO . BUT SHHH ! WHATEVER YOUR HIGHNESS COMMANDS ME TO DO , IT IS ALWAYS MY DUTY TO DO IT . ARE YOU GOING RIDING THIS AFTERNOON ?\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "LET HIM COME FORWARD .\n",
      "- context of x :\n",
      "THE MOST EVEN WHEN THEY’RE SAYING THE LEAST , IN MY OPINION . YOUR GRACE , THE PERSON WHO IS GOING TO DELIVER THE PROLOGUE IS READY . IF WE HAPPEN TO OFFEND YOU , IT’S BECAUSE WE WANT TO . WE DON’T WANT YOU TO THINK WE CAME HERE TO OFFEND YOU , EXCEPT THAT WE WANT\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "IT WEARIES ME ; YOU SAY IT WEARIES YOU .\n",
      "- context of x :\n",
      "IS SIGNOR ANTONIO . IN SOOTH , I KNOW NOT WHY I AM SO SAD . BELIEVE ME , SIR , HAD I SUCH VENTURE FORTH , THE BETTER PART OF MY AFFECTIONS WOULD BE WITH MY HOPES ABROAD . AND EVERY OBJECT THAT MIGHT MAKE ME FEAR MISFORTUNE TO MY VENTURES OUT OF DOUBT WOULD MAKE ME\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "WELL , GIRL , YOU’RE WEEPING NOT FOR HIS DEATH AS MUCH AS FOR THE FACT THAT THE VILLAIN WHO KILLED HIM IS STILL ALIVE .\n",
      "- context of x :\n",
      "NO , I’LL HELP TOO . WHAT’S THIS PART FOR ? YOU ARMOR MY HEART . NO !\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "WHICH ONE OF YOU KILLED THE DEER ?\n",
      "- context of x :\n",
      "I’M GOING TO FIND SOME SHADE AND SIGH UNTIL HE RETURNS . AND I’M GOING TO SLEEP . YES , SIR . THEN SING IT .\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "SHE IS OUR CAPITAL DEMAND , COMPRISED WITHIN THE FORERANK OF OUR ARTICLES .\n",
      "- context of x :\n",
      "MAY DO SOME GOOD , WHEN ARTICLES TOO NICELY URGED BE STOOD ON . YET LEAVE OUR COUSIN KATHERINE HERE WITH US . SHE HATH GOOD LEAVE . FAIR KATHERINE , AND MOST FAIR , WILL YOU VOUCHSAFE TO TEACH A SOLDIER TERMS SUCH AS WILL ENTER AT A LADY’S EAR AND PLEAD HIS LOVE SUIT TO\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "WHY , WHITHER , ADAM , WOULDST THOU HAVE ME GO ?\n",
      "- context of x :\n",
      "THIS IS NO PLACE , THIS HOUSE IS BUT A BUTCHERY . ABHOR IT , FEAR IT , DO NOT ENTER IT . NO MATTER WHITHER , SO YOU COME NOT HERE . WHAT , WOULDST THOU HAVE ME GO AND BEG MY FOOD , OR WITH A BASE AND BOIST'ROUS SWORD ENFORCE A THIEVISH LIVING ON THE COMMON ROAD ?\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "CASCA WILL TELL US WHAT THE MATTER IS .\n",
      "- context of x :\n",
      "LOOK YOU , CASSIUS , THE ANGRY SPOT DOTH GLOW ON CAESAR’S BROW , AND ALL THE REST LOOK LIKE A CHIDDEN TRAIN . CALPHURNIA’S CHEEK IS PALE , AND CICERO LOOKS WITH SUCH FERRET AND SUCH FIERY EYES AS WE HAVE SEEN HIM IN THE CAPITOL BEING CROSSED IN CONFERENCE BY SOME SENATORS . ANTONIO . CAESAR .\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "IF I QUENCH THEE , THOU FLAMING MINISTER , I CAN AGAIN THY FORMER LIGHT RESTORE SHOULD I REPENT ME .\n",
      "- context of x :\n",
      "THAT’S A DEGREE TO LOVE . NO , NOT A GRIZE . THERE LIES YOUR WAY , DUE WEST . STAY , I PRITHEE , TELL ME WHAT THOU THINKEST OF ME .\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "WE WILL HAVE NO TELLING .\n",
      "- context of x :\n",
      "WHAT THINK’ST THOU ? HAD WE FOUGHT , I DOUBT WE SHOULD HAVE BEEN TOO YOUNG FOR THEM . AS I AM AN HONEST MAN , HE LOOKS PALE . — ART THOU SICK , OR ANGRY ? KILLED A CAT ?\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "RICHARD EXCEPT , THOSE WHOM WE FIGHT AGAINST HAD RATHER HAVE US WIN THAN HIM THEY FOLLOW .\n",
      "- context of x :\n",
      "HIS SPEECH WAS LIKE A TANGLED CHAIN . WHO IS NEXT ? NO WONDER , MY LORD . WOULD YOU DESIRE LIME AND HAIR TO SPEAK BETTER ?\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "THIS BOX IS TOO THREATENING .\n",
      "- context of x :\n",
      "MAY THE GODS DESTROY YOU ! DO YOU MAINTAIN THE SAME STORY ? GO , GET OUT ! EVEN IF YOU WERE AS HANDSOME AS I BEG YOUR HIGHNESS’ PARDON .\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "GO , PREPARE FOR DINNER .\n",
      "- context of x :\n",
      "IT IS AN ACCUSTOMED ACTION WITH HER TO SEEM THUS WASHING HER HANDS . YET HERE’S A SPOT . SHE SPEAKS . I WILL SET DOWN WHAT COMES FROM HER , TO SATISFY MY REMEMBRANCE THE MORE STRONGLY .\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "WHEN ARE YOU MARRIED , MADAM ?\n",
      "- context of x :\n",
      "BUT HE HATH CHID ME HENCE AND THREATENED ME TO STRIKE ME , SPURN ME — NAY , TO KILL ME TOO . LET ME GO . WHY , GET YOU GONE ! WHO IS ’T THAT HINDERS YOU ?\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "WELCOME , MY LORD .\n",
      "- context of x :\n",
      "HURRY , GO UP TO THE ROOF . THE LORD MAYOR IS KNOCKING . I DON’T THINK HE WANTS TO BE DISTURBED . NOW , CATESBY , WHAT DOES YOUR LORD SAY TO MY REQUEST ?\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "WHY , WHAT IS THERE TO DO BEFORE I GET THERE ?\n",
      "- context of x :\n",
      "I’M OFF . WHAT WOULD YOU LIKE ME TO DO AT SALISBURY ? YOUR HIGHNESS JUST TOLD ME THAT I SHOULD RIDE THERE BEFORE YOU . I CHANGED MY MIND .\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "PLEASE DON’T PLAY GAMES .\n",
      "- context of x :\n",
      "GO ! DO YOU HEAR , MY FRIEND ? CASSIO SHE’S AWAKE , SIR . IF SHE FEELS LIKE COMING OVER HERE , I’LL GIVE HER YOUR MESSAGE .\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "AS I INTEND TO PROSPER AND REPENT , SO THRIVE I IN MY DANGEROUS AFFAIRS OF HOSTILE ARMS !\n",
      "- context of x :\n",
      "IS NOT TOMORROW , BOY , THE IDES OF MARCH ? I KNOW NOT , SIR . I WILL , SIR . SIR , MARCH IS WASTED FIFTEEN DAYS .\n",
      "- context label : 0\n"
     ]
    }
   ],
   "source": [
    "for x,y , ctx_x,ctx_y , len_x,len_y , len_ctx_x,len_ctx_y, label,label_ctx in train_loader:\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        print(\"\\n- x :\")\n",
    "        print(code2string(x[i],dict_token))\n",
    "        print(\"- context of x :\")\n",
    "        print(code2string(ctx_x[i],dict_token))\n",
    "        print(\"- context label :\",label_ctx[i].item())\n",
    "#         ipdb.set_trace()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adagrad\n",
    "from torch.nn import BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Adagrad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-446e48af38b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Adagrad' is not defined"
     ]
    }
   ],
   "source": [
    "epochs=100\n",
    "optimizer=Adagrad(params=model.parameters(),lr=0.01)\n",
    "loss_func=BCELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x,y , ctx_x,ctx_y , len_x,len_y , len_ctx_x,len_ctx_y, label,label_ctx in train_loader:\n",
    "        label_pred=model.forward(x)\n",
    "        loss=loss_func(label,label_pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch %10==0:\n",
    "        print(\"Epoch %d, loss %f\"%(epoch,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
