{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>PROJECT SANDBOX</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "The aim of this notebook is to provide a simple sandbox to test different NN architectures for the project. , here is a doc about the functions imported from `scripts` folder : \n",
    "\n",
    "- **`prepare_dataset(device,ratio=0.5,shuffle_ctx=False)`** :\n",
    "    - **Input**:\n",
    "        - device : a torch.device object\n",
    "        - ratio : a float ratio between 0 and 1 that determines the average proportion of modern english verses in the data loader\n",
    "        - shuffle_ctx : if `True`, shuffle the contexts within a Batch so that half of the `x_1` elements has a wrong context `ctx_1`. Useful to train the context recognizer model.\n",
    "    - **Return** :\n",
    "        - a torch Dataset | class : Shakespeare inherited from torch.utils.data.Dataset\n",
    "        - a python word dictionary (aka tokenizer) | class : dict\n",
    "    - **Tensors returned when loaded in the dataloader**:\n",
    "        - x_1 : input verse (modern / shakespearian)\n",
    "        - x_2 : output verse (modern / shakespearian)\n",
    "\n",
    "        - ctx_1 = context of the input verse\n",
    "        - ctx_2 = context of the output verse\n",
    "\n",
    "        - len_x : length of the input verse\n",
    "        - len_y : length of the output verse\n",
    "\n",
    "        - len_ctx_x : length of the input verse context\n",
    "        - len_ctx_y : length of the output verse context\n",
    "\n",
    "        - label : label of the input verse (0 : modern, 1 : shakespearian)\n",
    "        - label_ctx : label of the context (0 : wrong context, 1 : right context)\n",
    "- **`string2code(string,dict)`** : \n",
    "    - **Input**:\n",
    "        - string : a sentence\n",
    "        - dict : a tokenizer\n",
    "    - **Return** :\n",
    "        - a torch Longtensor (sentence tokenized)\n",
    "- **`code2string(torch.Longtensor,dict)`** : \n",
    "    - **Input**:\n",
    "        - torch.Longtensor : a sentence tokenized\n",
    "        - dict : a tokenizer\n",
    "    - **Return** :\n",
    "        - a string sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cpu\n"
     ]
    }
   ],
   "source": [
    "from scripts.data_builders.prepare_dataset import prepare_dataset,string2code,code2string,assemble\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import ipdb\n",
    "import pickle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device = \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ...\n",
      "- Shakespeare dataset length :  20316\n",
      "- Corrupted samples (ignored) :  763\n"
     ]
    }
   ],
   "source": [
    "train_data, dict_words = prepare_dataset(device,ratio=0.5,shuffle_ctx=True) #check with shift+tab to look at the data structure\n",
    "batch_size = 20\n",
    "dict_token = {b:a for a,b in dict_words.items()} #dict for code2string\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                           shuffle=True,collate_fn=train_data.collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size=len(dict_words) #19089\n",
    "d_embedding=300 #cf. paper Y.Kim 2014 Convolutional Neural Networks for Sentence Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoherenceClassifier(torch.nn.Module):\n",
    "    def __init__(self,dict_size=dict_size,d_embedding=300,max_length=100):\n",
    "        super().__init__()\n",
    "        self.embed_layer=torch.nn.Embedding(dict_size+1,d_embedding,padding_idx=dict_size)\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv1d(d_embedding,3,kernel_size = 3, stride = 1)\n",
    "        self.max_pool = torch.nn.MaxPool1d(3,2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear = torch.nn.Linear(3,1)\n",
    "        # self.f=lambda x: torch.norm(x,dim=1)**2 (I am not sure it is necessary at all)\n",
    "        self.f=lambda x:x\n",
    "        self.sigmoid=torch.nn.Sigmoid()\n",
    "#         self.softmax=torch.nn.Softmax()\n",
    "    \n",
    "    def forward(self,x,ctx):\n",
    "        x=self.embed_layer(x)\n",
    "        ctx=self.embed_layer(ctx)\n",
    "        input_=assemble(x,ctx,self.f)\n",
    "#         input_=torch.cat((self.f(x),ctx),dim=1)\n",
    "        input_ = self.conv_1(input_.transpose(1,2))\n",
    "        input_ = self.max_pool(input_)\n",
    "        input_ = self.relu(input_)\n",
    "        u = torch.max(input_,2)[0]\n",
    "        s = self.sigmoid(self.linear(u))\n",
    "        return(s)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- x :\n",
      "IT’S TOO BAD THEIR BEAUTY FADES RIGHT WHEN IT REACHES PERFECTION !\n",
      "- context of x :\n",
      "WOMEN ARE LIKE ROSES : THE MOMENT THEIR BEAUTY IS IN FULL BLOOM , IT’S ABOUT TO DECAY . THAT’S TRUE . WHO’S THE RULER HERE ? A DUKE WHO IS NOBLE IN NAME AND CHARACTER .\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "IT WOULD BE BAD IF SHE KNEW ABOUT BENEDICK’S LOVE AND TEASED HIM ABOUT IT .\n",
      "- context of x :\n",
      "THERE IS SCORN AND DISDAIN IN HER EYES , AND THOSE SPARKLING EYES DESPISE EVERYTHING THEY LOOK UPON . SHE CAN’T EVEN IMAGINE WHAT “LOVE” IS . IT’S TRUE . AND SO SHE TURNS MEN INSIDE OUT AND NEVER ACKNOWLEDGES THE INTEGRITY AND MERIT THAT A MAN HAS .\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "WHY SHOULD SHE WRITE TO EDMUND ?\n",
      "- context of x :\n",
      "COWARDS DIE MANY TIMES BEFORE THEIR DEATHS . THE VALIANT NEVER TASTE OF DEATH BUT ONCE . WHAT SAY THE AUGURERS ? THEY WOULD NOT HAVE YOU TO STIR FORTH TODAY .\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "SO THE SINS OF MY MOTHER SHOULD BE VISITED UPON ME .\n",
      "- context of x :\n",
      "HONORS ON THIS MAN TO EASE OURSELVES OF DIVERS SLANDEROUS LOADS , HE SHALL BUT BEAR THEM AS THE ASS BEARS GOLD , TO GROAN AND SWEAT UNDER THE BUSINESS , EITHER LED OR DRIVEN , AS WE POINT THE WAY . YOU MAY DO YOUR WILL , BUT HE’S A TRIED AND VALIANT SOLDIER . SO IS\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "I CANNOT BLAME YOU NOW FOR WEEPING .\n",
      "- context of x :\n",
      "MAYBE , BUT I WISH I’D NEVER LAID EYES ON HIM . GO , DAUGHTER . MASTER , MASTER ! I HAVE NEWS — OLD NEWS SUCH AS YOU NEVER HEARD BEFORE !\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "THE COUNT SENT ME AN EXCELLENT PAIR OF PERFUMED GLOVES .\n",
      "- context of x :\n",
      "YOU SING , AND I’LL DANCE . NEVER ! SORRY , I’M ALL STUFFED . EVER SINCE YOU LOST YOURS .\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "HOW COULD YOU ASK ?\n",
      "- context of x :\n",
      "I KNOW HIM WELL . HE’S HIS HOMELAND’S JEWEL . THAT’S THE RIGHT ANSWER — IT SHOWS YOUR LOVE . LET’S GO .\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "COME , MY SPADE .\n",
      "- context of x :\n",
      "WHY THEN , MY COUSIN , MARGARET , AND URSULA ARE MUCH DECEIVED , FOR THEY DID SWEAR YOU DID . THEY SWORE THAT YOU WERE ALMOST SICK FOR ME . TIS NO SUCH MATTER . THEN YOU DO NOT LOVE ME ?\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "AH , ALL YOU UP IN HEAVEN !\n",
      "- context of x :\n",
      "GOOD-BYE , GOOD-BYE , GOOD-BYE . REMEMBER ME . AND EARTH ! WHAT ELSE ?\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "HOW NOW , MALVOLIO ?\n",
      "- context of x :\n",
      "LEND ME THY CLOAK , SIR THOMAS . WE SHALL , MY LIEGE . NO , MY GOOD KNIGHT . GO WITH MY BROTHERS TO MY LORDS OF ENGLAND .\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "WHAT A NIGHT’S THIS !\n",
      "- context of x :\n",
      "HE SOUGHT MY LIFE , BUT LATELY , VERY LATE . I LOVED HIM , FRIEND — NO FATHER HIS SON DEARER . O , CRY YOUR MERCY , SIR . — TOM’S A-COLD . KEEP THEE WARM .\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "YOU TOO .\n",
      "- context of x :\n",
      "OH , SO CRUEL ! OH DEAR GODS ! I’VE SERVED YOU SINCE CHILDHOOD , BUT I’VE NEVER DONE YOU A BETTER SERVICE THAN TELLING YOU TO STOP . WHAT’S THIS , YOU DOG ?\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "IS THAT TRUE ?\n",
      "- context of x :\n",
      "THE SILK CAME FROM SACRED SILKWORMS , AND IT WAS DYED WITH FLUID MADE FROM EMBALMED VIRGINS' HEARTS . REALLY ? IT’S ABSOLUTELY TRUE , SO TAKE GOOD CARE OF IT . I WISH I HAD NEVER SEEN IT !\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "AY , BUT I KNOW — WHAT DOST THOU KNOW ?\n",
      "- context of x :\n",
      "THE LIVER , BUT THE PALATE , THAT SUFFER SURFEIT , CLOYMENT , AND REVOLT ; BUT MINE IS ALL AS HUNGRY AS THE SEA , AND CAN DIGEST AS MUCH . MAKE NO COMPARE BETWEEN THAT LOVE A WOMAN CAN BEAR ME AND THAT I OWE OLIVIA . TOO WELL WHAT LOVE WOMEN TO MEN MAY OWE . IN\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "PETRUCHIO , LET’S STAND OVER HERE AWHILE .\n",
      "- context of x :\n",
      "I’M GLAD TO SEE YOU IN SUCH A MERRY MOOD . BUT WHAT’S THE JOKE ? WHAT , ARE YOU MOCKING ME TO MY FACE ? HERE .\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "I HAVE NOTHING TO DO WITH WHAT YOU’RE TALKING ABOUT , MY LORD .\n",
      "- context of x :\n",
      "IT’S NINE O'CLOCK ! OUR FRIENDS ARE ALL WAITING FOR YOU . I’VE SENT TWENTY PEOPLE TO LOOK FOR YOU . I’M GLAD .\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "THE HIGHER NILUS SWELLS THE MORE IT PROMISES .\n",
      "- context of x :\n",
      "BID ME TEAR THE BOND . WHEN IT IS PAID ACCORDING TO THE TENOR . YOU KNOW THE LAW . I CHARGE YOU BY THE LAW , WHEREOF YOU ARE A WELL-DESERVING PILLAR , PROCEED TO JUDGMENT .\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "COME , BRING ME UNTO MY CHANCE .\n",
      "- context of x :\n",
      "BUT , ALAS THE WHILE ! NOR WILL NOT . FIRST , FORWARD TO THE TEMPLE . AFTER DINNER YOUR HAZARD SHALL BE MADE .\n",
      "- context label : 1\n",
      "\n",
      "- x :\n",
      "I HAD MY FATHER’S SIGNET IN MY PURSE , WHICH WAS THE MODEL OF THAT DANISH SEAL .\n",
      "- context of x :\n",
      "THEREFORE PREPARE THEE TO CUT OFF THE FLESH . SHED THOU NO BLOOD , NOR CUT THOU LESS NOR MORE BUT JUST A POUND OF FLESH . A SECOND DANIEL ! — A DANIEL , JEW ! NOW , INFIDEL , I HAVE YOU ON THE HIP .\n",
      "- context label : 0\n",
      "\n",
      "- x :\n",
      "GO ON , WEAR YOUR BOOTS OUT !\n",
      "- context of x :\n",
      "THEY’VE EATEN THEMSELVES SICK . THE DOOR IS OPEN , SIR . AS FOR ME , I’LL LEAVE WHEN I LIKE . CALM DOWN , KATE .\n",
      "- context label : 1\n"
     ]
    }
   ],
   "source": [
    "for x,y , ctx_x,ctx_y , len_x,len_y , len_ctx_x,len_ctx_y, label,label_ctx in train_loader:\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        print(\"\\n- x :\")\n",
    "        print(code2string(x[i],dict_token))\n",
    "        print(\"- context of x :\")\n",
    "        print(code2string(ctx_x[i],dict_token))\n",
    "        print(\"- context label :\",label_ctx[i].item())\n",
    "#         ipdb.set_trace()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adagrad\n",
    "from torch.nn import BCELoss,CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/default/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 0.608942\n"
     ]
    }
   ],
   "source": [
    "epochs=100\n",
    "model=CoherenceClassifier()\n",
    "optimizer=Adagrad(params=model.parameters(),lr=0.01)\n",
    "loss_func=BCELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x,y , ctx_x,ctx_y , len_x,len_y , len_ctx_x,len_ctx_y, label,label_ctx in train_loader:\n",
    "        x[x==-1]=19089\n",
    "        ctx_x[ctx_x==-1]=19089\n",
    "#         ipdb.set_trace()\n",
    "        label_pred=model.forward(x,ctx_x)\n",
    "        loss=loss_func(label_pred,label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch %1==0:\n",
    "        print(\"Epoch %d, loss %f\"%(epoch,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
