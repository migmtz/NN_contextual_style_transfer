{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>PROJECT SANDBOX</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "The aim of this notebook is to provide a simple sandbox to test different NN architectures for the project. , here is a doc about the functions imported from `scripts` folder : \n",
    "\n",
    "- **`prepare_dataset(device,ratio=0.5,shuffle_ctx=False)`** :\n",
    "    - **Input**:\n",
    "        - device : a torch.device object\n",
    "        - ratio : a float ratio between 0 and 1 that determines the average proportion of modern english verses in the data loader\n",
    "        - shuffle_ctx : if `True`, shuffle the contexts within a Batch so that half of the `x_1` elements has a wrong context `ctx_1`. Useful to train the context recognizer model.\n",
    "    - **Return** :\n",
    "        - a torch Dataset | class : Shakespeare inherited from torch.utils.data.Dataset\n",
    "        - a python word dictionary (aka tokenizer) | class : dict\n",
    "    - **Tensors returned when loaded in the dataloader**:\n",
    "        - x_1 : input verse (modern / shakespearian)\n",
    "        - x_2 : output verse (modern / shakespearian)\n",
    "\n",
    "        - ctx_1 = context of the input verse\n",
    "        - ctx_2 = context of the output verse\n",
    "\n",
    "        - len_x : length of the input verse\n",
    "        - len_y : length of the output verse\n",
    "\n",
    "        - len_ctx_x : length of the input verse context\n",
    "        - len_ctx_y : length of the output verse context\n",
    "\n",
    "        - label : label of the input verse (0 : modern, 1 : shakespearian)\n",
    "        - label_ctx : label of the context (0 : wrong context, 1 : right context)\n",
    "- **`string2code(string,dict)`** : \n",
    "    - **Input**:\n",
    "        - string : a sentence\n",
    "        - dict : a tokenizer\n",
    "    - **Return** :\n",
    "        - a torch Longtensor (sentence tokenized)\n",
    "- **`code2string(torch.Longtensor,dict)`** : \n",
    "    - **Input**:\n",
    "        - torch.Longtensor : a sentence tokenized\n",
    "        - dict : a tokenizer\n",
    "    - **Return** :\n",
    "        - a string sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "from scripts.data_builders.prepare_dataset import prepare_dataset,string2code,code2string,assemble\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pickle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device = \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ...\n",
      "- Shakespeare dataset length :  21079\n",
      "- Corrupted samples (ignored) :  0\n"
     ]
    }
   ],
   "source": [
    "train_data, dict_words = prepare_dataset(device,ratio=0.5,shuffle_ctx=True) #check with shift+tab to look at the data structure\n",
    "batch_size = 128\n",
    "dict_token = {b:a for a,b in dict_words.items()} #dict for code2string\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                           shuffle=True,collate_fn=train_data.collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(dict_words) #19089\n",
    "d_embedding = 300 #cf. paper Y.Kim 2014 Convolutional Neural Networks for Sentence Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoherenceClassifier(torch.nn.Module):\n",
    "    def __init__(self,dict_size=dict_size,d_embedding=300):\n",
    "        super().__init__()\n",
    "        self.embed_layer=torch.nn.Embedding(dict_size+1,d_embedding,padding_idx=dict_size)\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv1d(d_embedding,3,kernel_size = 3, stride = 1)\n",
    "        self.max_pool = torch.nn.MaxPool1d(3,2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear = torch.nn.Linear(3,1)\n",
    "        # self.f=lambda x: torch.norm(x,dim=1)**2 (I am not sure it is necessary at all)\n",
    "        self.f = lambda x:x\n",
    "        self.sigmoid=torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x,ctx):\n",
    "        x = self.embed_layer(x)\n",
    "        ctx = self.embed_layer(ctx)\n",
    "        x = torch.cat((self.f(x),ctx),dim=1)\n",
    "        x = self.conv_1( x.transpose(1,2))\n",
    "        x = self.max_pool( x )\n",
    "        x = self.relu( x )\n",
    "        x = torch.max( x , 2 )[0]\n",
    "        x = self.sigmoid(self.linear(x))\n",
    "        return(x)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoherenceClassifier(torch.nn.Module):\n",
    "    def __init__(self,dict_size=dict_size,d_embedding=300,d_hidden=100):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.embedding = nn.Embedding(dict_size+1,d_embedding,padding_idx=dict_size)\n",
    "        self.lstm = nn.LSTM(d_embedding,self.d_hidden,dropout=0.,num_layers=1,bidirectional=False)\n",
    "        self.linear = torch.nn.Linear(self.d_hidden,1)\n",
    "    \n",
    "    def forward(self,x,len_x):\n",
    "        x = self.embedding(x)\n",
    "        x = pack_padded_sequence(x.permute(1,0,2),len_x,enforce_sorted=False)\n",
    "        _,x = self.lstm(x)\n",
    "        x = x[0].reshape(-1,self.d_hidden)\n",
    "        x = torch.sigmoid( self.linear(x) ).reshape(-1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y , ctx_x,ctx_y , len_x,len_y , len_ctx_x,len_ctx_y, label,label_ctx in train_loader:\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        print(\"\\n- x :\")\n",
    "        print(code2string(x[i],dict_token))\n",
    "        print(\"- context of x :\")\n",
    "        print(code2string(ctx_x[i],dict_token))\n",
    "        print(\"- context label :\",label_ctx[i].item())\n",
    "\n",
    "        print(\"- len_ctx_x :\")\n",
    "        print(ctx_x)\n",
    "#         ipdb.set_trace()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "model=CoherenceClassifier().to(device)\n",
    "optimizer= optim.Adam(params=model.parameters(),lr=0.001)\n",
    "loss_func=BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 0.69419\n",
      "1 \t 0.69354\n",
      "2 \t 0.69355\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f749cee79903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = len(train_data.x) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x,y , ctx_x,ctx_y , len_x,len_y , len_ctx_x,len_ctx_y, label,label_ctx in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = model.forward(x,len_x)\n",
    "        loss = loss_func(x,label_ctx.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(epoch,\"\\t\",round(total_loss/n,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
