{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Parallel Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Python Notebook concerning the final version containing the coding for the Only-Parallel (Supervised) version of the program. It is to be completed with the correct format of the Contextual and Style Classifiers in their corresponding parts.\n",
    "\n",
    "Date of upload: Friday 31th January\n",
    "\n",
    "Actual Version: 3.0, coherence implemented (Monday 10th February, night)\n",
    "\n",
    "Precedent Versions : 2.1, 2.0, 1.1, 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n",
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "from scripts.data_builders.prepare_dataset import prepare_dataset_parallel,string2code,code2string\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchnlp.metrics import get_moses_multi_bleu,get_token_accuracy\n",
    "from torch.optim import Adam\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device = \",device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ...\n",
      "- Shakespeare dataset length :  21079\n",
      "- Corrupted samples (ignored) :  0\n"
     ]
    }
   ],
   "source": [
    "train_data, dict_words = prepare_dataset_parallel(\"data/shakespeare.csv\",device,ratio=0.5) #check with shift+tab to look at the data structure\n",
    "dict_token = {b:a for a,b in dict_words.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = Path(\"data/models/embedding_v1\")\n",
    "embedding = torch.load(savepath,map_location = device)\n",
    "embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_heads = 4\n",
    "d_feedforward = 1024\n",
    "batch_size = 32\n",
    "dict_size = len(dict_token)\n",
    "\n",
    "d_embedding = embedding.embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Classifier (To be filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath_style = Path(\"data/models/coherence_classifier_v1_epoch_100\")\n",
    "coherence_classifier = torch.load(savepath_style,map_location = device)\n",
    "coherence_classifier.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style classifier (To be filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath_style = Path(\"data/models/style_classifier_v1\")\n",
    "style_classifier = torch.load(savepath_style,map_location = device)\n",
    "style_classifier.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (To be adapted once we have the Context and Style Class For the Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelModel(torch.nn.Module):\n",
    "    def __init__(self,dict_size, d_embedding, nb_heads, d_feedforward):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.embed_layer = torch.nn.Embedding(dict_size+1, d_embedding, padding_idx=dict_size)\n",
    "        self.positional_layer = PositionalEncoding(d_embedding)\n",
    "        self.sentence_encoder = torch.nn.TransformerEncoderLayer(d_model = d_embedding, nhead = nb_heads,\n",
    "                                                    dim_feedforward = d_feedforward)\n",
    "        self.context_encoder = torch.nn.TransformerEncoderLayer(d_model = d_embedding, nhead = nb_heads,\n",
    "                                                    dim_feedforward = d_feedforward)\n",
    "        self.sentence_decoder = torch.nn.TransformerDecoderLayer(d_model = d_embedding, nhead = nb_heads,\n",
    "                                                    dim_feedforward = d_feedforward)\n",
    "        self.label_embedding = torch.nn.Embedding(2,768)\n",
    "        self.padd = dict_size\n",
    "   \n",
    "    def _generate_padding_mask(self,x):\n",
    "        mask = (x == dict_size)\n",
    "        return mask\n",
    "   \n",
    "    def _generate_square_subsequent_mask(self,sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "   \n",
    "    def forward(self,x,ctx_x,y,label_x):\n",
    "        device = x.device\n",
    "        padd_x_mask = self._generate_padding_mask(x).to(device)\n",
    "        padd_ctx_mask = self._generate_padding_mask(ctx_x).to(device)\n",
    "        padd_y_mask = self._generate_padding_mask(y).to(device)\n",
    "       \n",
    "        mask_x = self._generate_square_subsequent_mask(x.shape[1]).to(device)\n",
    "        mask_ctx = self._generate_square_subsequent_mask(ctx_x.shape[1]).to(device)\n",
    "        mask_y = self._generate_square_subsequent_mask(y.shape[1]).to(device)\n",
    "       \n",
    "        # Embedding\n",
    "        x = self.embed_layer(x).transpose(0,1) # token x batch x embedding\n",
    "        ctx_x = self.embed_layer(ctx_x).transpose(0,1)\n",
    "       \n",
    "        # Positional Encoding\n",
    "        x = self.positional_layer(x)\n",
    "        ctx_x = self.positional_layer(ctx_x)        \n",
    "       \n",
    "        # Encoders\n",
    "        x_enc = self.sentence_encoder(x,src_mask=mask_x,src_key_padding_mask=padd_x_mask)\n",
    "        ctx_enc = self.context_encoder(ctx_x,src_mask=mask_ctx,src_key_padding_mask=padd_ctx_mask)\n",
    " \n",
    "        # Linear and Style Mixing\n",
    "        x_and_ctx = torch.cat((x_enc,ctx_enc),dim = 0)\n",
    "        label = (1-label_x).reshape((1,x_and_ctx.shape[1])).expand((x_and_ctx.shape[0],x_and_ctx.shape[1])).to(device)\n",
    "        x_lab = x_and_ctx + self.label_embedding(label)\n",
    "       \n",
    "        # Decoder\n",
    "        padd_mem_mask = torch.cat((padd_x_mask,padd_ctx_mask),1)\n",
    "        y = self.embed_layer(y)\n",
    "        y_pos = self.positional_layer(y.transpose(0,1))\n",
    "        y_pred = self.sentence_decoder(y_pos,x_lab,tgt_mask=mask_y,tgt_key_padding_mask=padd_y_mask,\n",
    "                                       memory_key_padding_mask=padd_mem_mask)\n",
    "       \n",
    "        return(y_pred.transpose(0,1),y)\n",
    "   \n",
    "    def translator(self,x,ctx_x,y,label_x):\n",
    "        device = x.device\n",
    "        mask_x = self._generate_square_subsequent_mask(x.shape[1]).to(device)\n",
    "        mask_ctx = self._generate_square_subsequent_mask(ctx_x.shape[1]).to(device)\n",
    "       \n",
    "        # Embedding\n",
    "        x = self.embed_layer(x).transpose(0,1) # token x batch x embedding\n",
    "        ctx_x = self.embed_layer(ctx_x).transpose(0,1)\n",
    "       \n",
    "        # Positional Encoding\n",
    "        x = self.positional_layer(x)\n",
    "        ctx_x = self.positional_layer(ctx_x)        \n",
    "       \n",
    "        # Encoders\n",
    "        x_enc = self.sentence_encoder(x,mask_x)\n",
    "        ctx_enc = self.context_encoder(ctx_x,mask_ctx)\n",
    "       \n",
    "        # Linear and Style Mixing\n",
    "        x_and_ctx = torch.cat((x_enc,ctx_enc),dim = 0)\n",
    "        label = (1-label_x).reshape((1,x_and_ctx.shape[1])).expand((x_and_ctx.shape[0],x_and_ctx.shape[1])).to(device)\n",
    "        x_lab = x_and_ctx + self.label_embedding(label)\n",
    "        return(x_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training  (To be adapted once we have the Context and Style Class For the Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the model(s)\n",
    "\n",
    "#model = ParallelModel(dict_size, d_embedding, nb_heads, d_feedforward).to(device)\n",
    "model = torch.load(\"data/models/parallel_model_v0_epoch_200\",map_location = device)\n",
    "model.embed_layer = embedding\n",
    "model.embed_layer.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information concerning the Training optimizer\n",
    "\n",
    "decoder_linear = torch.load(\"data/models/parallel_decoder_linear_v0_epoch_200\",map_location = device)\n",
    "softmax_layer = torch.nn.LogSoftmax(dim = 2).to(device)\n",
    "\n",
    "params = list(model.parameters()) + list(decoder_linear.parameters())#+ list(context_encoder.parameters()) + \n",
    "                                  #list(linear_context.parameters) + list(sentence_decoder.parameters())\n",
    "\n",
    "l_r = 1e-5\n",
    "optimizer=Adam(params,lr=l_r)\n",
    "\n",
    "#Weights of the losses\n",
    "l1=1 #\n",
    "l2=1\n",
    "l3=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses \n",
    "#loss_seq2seq = torch.nn.SmoothL1Loss(reduction='mean') #Contextual Seq2Seq Loss\n",
    "#loss_seq2seq = torch.nn.KLDivLoss(reduction = 'mean')\n",
    "loss_seq2seq = torch.nn.CrossEntropyLoss(reduction = 'mean',ignore_index=dict_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 201 |     1/  658 batches | total loss  1.16 | seq2seq loss  1.15 | style loss  0.00 | coherence loss  0.01| total accuracy  0.29 | total BLEU 42.46\n",
      "| epoch 201 |    95/  658 batches | total loss  1.13 | seq2seq loss  1.12 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 37.94\n",
      "| epoch 201 |   189/  658 batches | total loss  1.13 | seq2seq loss  1.12 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 38.07\n",
      "| epoch 201 |   283/  658 batches | total loss  1.13 | seq2seq loss  1.13 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 38.03\n",
      "| epoch 201 |   377/  658 batches | total loss  1.13 | seq2seq loss  1.13 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 38.15\n",
      "| epoch 201 |   471/  658 batches | total loss  1.13 | seq2seq loss  1.13 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 38.14\n",
      "| epoch 201 |   565/  658 batches | total loss  1.13 | seq2seq loss  1.13 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 38.07\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 202 |     1/  658 batches | total loss  0.94 | seq2seq loss  0.93 | style loss  0.00 | coherence loss  0.01| total accuracy  0.29 | total BLEU 48.11\n",
      "| epoch 202 |    95/  658 batches | total loss  1.13 | seq2seq loss  1.12 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 38.72\n",
      "| epoch 202 |   189/  658 batches | total loss  1.13 | seq2seq loss  1.12 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 38.88\n",
      "| epoch 202 |   283/  658 batches | total loss  1.13 | seq2seq loss  1.12 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 38.78\n",
      "| epoch 202 |   377/  658 batches | total loss  1.12 | seq2seq loss  1.12 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 38.85\n",
      "| epoch 202 |   471/  658 batches | total loss  1.12 | seq2seq loss  1.12 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 38.84\n",
      "| epoch 202 |   565/  658 batches | total loss  1.12 | seq2seq loss  1.12 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 38.75\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 203 |     1/  658 batches | total loss  1.01 | seq2seq loss  1.00 | style loss  0.00 | coherence loss  0.01| total accuracy  0.37 | total BLEU 41.58\n",
      "| epoch 203 |    95/  658 batches | total loss  1.11 | seq2seq loss  1.11 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 38.84\n",
      "| epoch 203 |   189/  658 batches | total loss  1.11 | seq2seq loss  1.11 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 38.91\n",
      "| epoch 203 |   283/  658 batches | total loss  1.11 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 38.99\n",
      "| epoch 203 |   377/  658 batches | total loss  1.11 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.15\n",
      "| epoch 203 |   471/  658 batches | total loss  1.11 | seq2seq loss  1.11 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.01\n",
      "| epoch 203 |   565/  658 batches | total loss  1.11 | seq2seq loss  1.11 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-bleu.perl script returned non-zero exit code\n",
      "b'\\ngzip: /tmp/tmp_0u8k8gz: not in gzip format\\nERROR: could not find reference file /tmp/tmp_0u8k8gz at /tmp/tmpzoyxz7rq line 32.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 204 |     1/  658 batches | total loss  1.24 | seq2seq loss  1.23 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 35.92\n",
      "| epoch 204 |    95/  658 batches | total loss  1.12 | seq2seq loss  1.11 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 38.55\n",
      "| epoch 204 |   189/  658 batches | total loss  1.12 | seq2seq loss  1.11 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 38.94\n",
      "| epoch 204 |   283/  658 batches | total loss  1.12 | seq2seq loss  1.11 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 38.78\n",
      "| epoch 204 |   377/  658 batches | total loss  1.11 | seq2seq loss  1.11 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 38.86\n",
      "| epoch 204 |   471/  658 batches | total loss  1.11 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.01\n",
      "| epoch 204 |   565/  658 batches | total loss  1.11 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.01\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 205 |     1/  658 batches | total loss  1.00 | seq2seq loss  0.99 | style loss  0.00 | coherence loss  0.01| total accuracy  0.21 | total BLEU 40.65\n",
      "| epoch 205 |    95/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.65\n",
      "| epoch 205 |   189/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.68\n",
      "| epoch 205 |   283/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.46\n",
      "| epoch 205 |   377/  658 batches | total loss  1.10 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.36\n",
      "| epoch 205 |   471/  658 batches | total loss  1.10 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.36\n",
      "| epoch 205 |   565/  658 batches | total loss  1.11 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.37\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type ParallelModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 206 |     1/  658 batches | total loss  0.90 | seq2seq loss  0.90 | style loss  0.00 | coherence loss  0.01| total accuracy  0.32 | total BLEU 43.43\n",
      "| epoch 206 |    95/  658 batches | total loss  1.08 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.77\n",
      "| epoch 206 |   189/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.33\n",
      "| epoch 206 |   283/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.47\n",
      "| epoch 206 |   377/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.35\n",
      "| epoch 206 |   471/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.23\n",
      "| epoch 206 |   565/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.18\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 207 |     1/  658 batches | total loss  1.19 | seq2seq loss  1.18 | style loss  0.00 | coherence loss  0.01| total accuracy  0.27 | total BLEU 36.18\n",
      "| epoch 207 |    95/  658 batches | total loss  1.11 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 39.03\n",
      "| epoch 207 |   189/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 39.53\n",
      "| epoch 207 |   283/  658 batches | total loss  1.11 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 39.38\n",
      "| epoch 207 |   377/  658 batches | total loss  1.10 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 39.43\n",
      "| epoch 207 |   471/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.43\n",
      "| epoch 207 |   565/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.34\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 208 |     1/  658 batches | total loss  0.89 | seq2seq loss  0.88 | style loss  0.00 | coherence loss  0.01| total accuracy  0.28 | total BLEU 44.98\n",
      "| epoch 208 |    95/  658 batches | total loss  1.11 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-bleu.perl script returned non-zero exit code\n",
      "b'\\ngzip: /tmp/tmpgnr6e5gz: not in gzip format\\nERROR: could not find reference file /tmp/tmpgnr6e5gz at /tmp/tmpl3k93bpn line 32.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 208 |   189/  658 batches | total loss  1.10 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.09\n",
      "| epoch 208 |   283/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.14\n",
      "| epoch 208 |   377/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.23\n",
      "| epoch 208 |   471/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.24\n",
      "| epoch 208 |   565/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.27\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 209 |     1/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.13 | total BLEU 39.64\n",
      "| epoch 209 |    95/  658 batches | total loss  1.08 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.82\n",
      "| epoch 209 |   189/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.74\n",
      "| epoch 209 |   283/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.67\n",
      "| epoch 209 |   377/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.56\n",
      "| epoch 209 |   471/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.54\n",
      "| epoch 209 |   565/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.41\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 210 |     1/  658 batches | total loss  0.96 | seq2seq loss  0.95 | style loss  0.00 | coherence loss  0.01| total accuracy  0.14 | total BLEU 47.71\n",
      "| epoch 210 |    95/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.06\n",
      "| epoch 210 |   189/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-bleu.perl script returned non-zero exit code\n",
      "b'\\ngzip: /tmp/tmppiij4ygz: not in gzip format\\nERROR: could not find reference file /tmp/tmppiij4ygz at /tmp/tmpxj6t_1pl line 32.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 210 |   283/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.31\n",
      "| epoch 210 |   377/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.22\n",
      "| epoch 210 |   471/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.26\n",
      "| epoch 210 |   565/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.32\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 211 |     1/  658 batches | total loss  1.29 | seq2seq loss  1.29 | style loss  0.00 | coherence loss  0.01| total accuracy  0.27 | total BLEU 38.12\n",
      "| epoch 211 |    95/  658 batches | total loss  1.10 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.33\n",
      "| epoch 211 |   189/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.45\n",
      "| epoch 211 |   283/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.45\n",
      "| epoch 211 |   377/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.42\n",
      "| epoch 211 |   471/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.44\n",
      "| epoch 211 |   565/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.46\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 212 |     1/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.15 | total BLEU 39.27\n",
      "| epoch 212 |    95/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.48\n",
      "| epoch 212 |   189/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.21\n",
      "| epoch 212 |   283/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.16\n",
      "| epoch 212 |   377/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.13\n",
      "| epoch 212 |   471/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.24\n",
      "| epoch 212 |   565/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.23\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 213 |     1/  658 batches | total loss  0.94 | seq2seq loss  0.94 | style loss  0.00 | coherence loss  0.01| total accuracy  0.28 | total BLEU 48.22\n",
      "| epoch 213 |    95/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.38\n",
      "| epoch 213 |   189/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.42\n",
      "| epoch 213 |   283/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.31\n",
      "| epoch 213 |   377/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.48\n",
      "| epoch 213 |   471/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.58\n",
      "| epoch 213 |   565/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.66\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 214 |     1/  658 batches | total loss  1.03 | seq2seq loss  1.02 | style loss  0.00 | coherence loss  0.01| total accuracy  0.28 | total BLEU 38.61\n",
      "| epoch 214 |    95/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 39.67\n",
      "| epoch 214 |   189/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 39.48\n",
      "| epoch 214 |   283/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 39.55\n",
      "| epoch 214 |   377/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 39.54\n",
      "| epoch 214 |   471/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.58\n",
      "| epoch 214 |   565/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.61\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 215 |     1/  658 batches | total loss  1.20 | seq2seq loss  1.19 | style loss  0.00 | coherence loss  0.01| total accuracy  0.30 | total BLEU 38.63\n",
      "| epoch 215 |    95/  658 batches | total loss  1.11 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 38.86\n",
      "| epoch 215 |   189/  658 batches | total loss  1.11 | seq2seq loss  1.10 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 38.73\n",
      "| epoch 215 |   283/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 39.15\n",
      "| epoch 215 |   377/  658 batches | total loss  1.10 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.17\n",
      "| epoch 215 |   471/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.40\n",
      "| epoch 215 |   565/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.41\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 216 |     1/  658 batches | total loss  1.02 | seq2seq loss  1.01 | style loss  0.00 | coherence loss  0.00| total accuracy  0.31 | total BLEU 40.08\n",
      "| epoch 216 |    95/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 39.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-bleu.perl script returned non-zero exit code\n",
      "b'\\ngzip: /tmp/tmp6ztcf1gz: not in gzip format\\nERROR: could not find reference file /tmp/tmp6ztcf1gz at /tmp/tmphyd28t3r line 32.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 216 |   189/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.82\n",
      "| epoch 216 |   283/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.63\n",
      "| epoch 216 |   377/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.65\n",
      "| epoch 216 |   471/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-bleu.perl script returned non-zero exit code\n",
      "b'\\ngzip: /tmp/tmpszpr8ygz: not in gzip format\\nERROR: could not find reference file /tmp/tmpszpr8ygz at /tmp/tmpa5zxehjd line 32.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 216 |   565/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.72\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 217 |     1/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.19 | total BLEU 39.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-bleu.perl script returned non-zero exit code\n",
      "b'\\ngzip: /tmp/tmp38h8dmgz: not in gzip format\\nERROR: could not find reference file /tmp/tmp38h8dmgz at /tmp/tmpjtset_hm line 32.\\n'\n",
      "multi-bleu.perl script returned non-zero exit code\n",
      "b'\\ngzip: /tmp/tmpjym0f3gz: not in gzip format\\nERROR: could not find reference file /tmp/tmpjym0f3gz at /tmp/tmpw13g3xhp line 32.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 217 |    95/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 38.72\n",
      "| epoch 217 |   189/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.14\n",
      "| epoch 217 |   283/  658 batches | total loss  1.08 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.31\n",
      "| epoch 217 |   377/  658 batches | total loss  1.08 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.33\n",
      "| epoch 217 |   471/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.29\n",
      "| epoch 217 |   565/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-bleu.perl script returned non-zero exit code\n",
      "b'\\ngzip: /tmp/tmprgi29rgz: not in gzip format\\nERROR: could not find reference file /tmp/tmprgi29rgz at /tmp/tmpa09ajtw_ line 32.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 218 |     1/  658 batches | total loss  0.97 | seq2seq loss  0.97 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 45.31\n",
      "| epoch 218 |    95/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.13\n",
      "| epoch 218 |   189/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.06\n",
      "| epoch 218 |   283/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.85\n",
      "| epoch 218 |   377/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.87\n",
      "| epoch 218 |   471/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.80\n",
      "| epoch 218 |   565/  658 batches | total loss  1.08 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.70\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 219 |     1/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.26 | total BLEU 42.16\n",
      "| epoch 219 |    95/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.38\n",
      "| epoch 219 |   189/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.36\n",
      "| epoch 219 |   283/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.35\n",
      "| epoch 219 |   377/  658 batches | total loss  1.09 | seq2seq loss  1.09 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.34\n",
      "| epoch 219 |   471/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.48\n",
      "| epoch 219 |   565/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.47\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 220 |     1/  658 batches | total loss  1.20 | seq2seq loss  1.20 | style loss  0.00 | coherence loss  0.01| total accuracy  0.30 | total BLEU 38.78\n",
      "| epoch 220 |    95/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.30\n",
      "| epoch 220 |   189/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.11\n",
      "| epoch 220 |   283/  658 batches | total loss  1.08 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.07\n",
      "| epoch 220 |   377/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.05\n",
      "| epoch 220 |   471/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.98\n",
      "| epoch 220 |   565/  658 batches | total loss  1.08 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.94\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 221 |     1/  658 batches | total loss  1.30 | seq2seq loss  1.30 | style loss  0.00 | coherence loss  0.01| total accuracy  0.21 | total BLEU 33.48\n",
      "| epoch 221 |    95/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.07\n",
      "| epoch 221 |   189/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.08\n",
      "| epoch 221 |   283/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.33\n",
      "| epoch 221 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.24\n",
      "| epoch 221 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.07\n",
      "| epoch 221 |   565/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.01\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 222 |     1/  658 batches | total loss  1.20 | seq2seq loss  1.20 | style loss  0.00 | coherence loss  0.01| total accuracy  0.21 | total BLEU 38.85\n",
      "| epoch 222 |    95/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 40.30\n",
      "| epoch 222 |   189/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 40.43\n",
      "| epoch 222 |   283/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.16\n",
      "| epoch 222 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.19\n",
      "| epoch 222 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.20\n",
      "| epoch 222 |   565/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.15\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 223 |     1/  658 batches | total loss  1.21 | seq2seq loss  1.20 | style loss  0.00 | coherence loss  0.01| total accuracy  0.20 | total BLEU 36.26\n",
      "| epoch 223 |    95/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.26\n",
      "| epoch 223 |   189/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.36\n",
      "| epoch 223 |   283/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 40.19\n",
      "| epoch 223 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.34\n",
      "| epoch 223 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.25\n",
      "| epoch 223 |   565/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.09\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 224 |     1/  658 batches | total loss  0.85 | seq2seq loss  0.84 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 47.17\n",
      "| epoch 224 |    95/  658 batches | total loss  1.08 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.82\n",
      "| epoch 224 |   189/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.86\n",
      "| epoch 224 |   283/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.97\n",
      "| epoch 224 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.09\n",
      "| epoch 224 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.01\n",
      "| epoch 224 |   565/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-bleu.perl script returned non-zero exit code\n",
      "b'\\ngzip: /tmp/tmpu3romugz: not in gzip format\\nERROR: could not find reference file /tmp/tmpu3romugz at /tmp/tmp053n5mfj line 32.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 225 |     1/  658 batches | total loss  1.15 | seq2seq loss  1.15 | style loss  0.00 | coherence loss  0.01| total accuracy  0.19 | total BLEU 39.15\n",
      "| epoch 225 |    95/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 40.14\n",
      "| epoch 225 |   189/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.94\n",
      "| epoch 225 |   283/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.87\n",
      "| epoch 225 |   377/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.84\n",
      "| epoch 225 |   471/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.88\n",
      "| epoch 225 |   565/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.87\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 226 |     1/  658 batches | total loss  1.19 | seq2seq loss  1.18 | style loss  0.00 | coherence loss  0.01| total accuracy  0.22 | total BLEU 41.16\n",
      "| epoch 226 |    95/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 40.22\n",
      "| epoch 226 |   189/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.24\n",
      "| epoch 226 |   283/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.34\n",
      "| epoch 226 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.03\n",
      "| epoch 226 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.99\n",
      "| epoch 226 |   565/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.99\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 227 |     1/  658 batches | total loss  1.25 | seq2seq loss  1.25 | style loss  0.00 | coherence loss  0.01| total accuracy  0.22 | total BLEU 37.10\n",
      "| epoch 227 |    95/  658 batches | total loss  1.08 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.17\n",
      "| epoch 227 |   189/  658 batches | total loss  1.08 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.97\n",
      "| epoch 227 |   283/  658 batches | total loss  1.08 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.92\n",
      "| epoch 227 |   377/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.08\n",
      "| epoch 227 |   471/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.97\n",
      "| epoch 227 |   565/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.92\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 228 |     1/  658 batches | total loss  1.01 | seq2seq loss  1.01 | style loss  0.00 | coherence loss  0.01| total accuracy  0.32 | total BLEU 41.11\n",
      "| epoch 228 |    95/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.35\n",
      "| epoch 228 |   189/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.48\n",
      "| epoch 228 |   283/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.35\n",
      "| epoch 228 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.22\n",
      "| epoch 228 |   471/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.99\n",
      "| epoch 228 |   565/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.08\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 229 |     1/  658 batches | total loss  1.11 | seq2seq loss  1.11 | style loss  0.00 | coherence loss  0.00| total accuracy  0.27 | total BLEU 39.82\n",
      "| epoch 229 |    95/  658 batches | total loss  1.09 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.17\n",
      "| epoch 229 |   189/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.96\n",
      "| epoch 229 |   283/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.09\n",
      "| epoch 229 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.12\n",
      "| epoch 229 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-bleu.perl script returned non-zero exit code\n",
      "b'\\ngzip: /tmp/tmpmz298zgz: not in gzip format\\nERROR: could not find reference file /tmp/tmpmz298zgz at /tmp/tmpyl5_l0yy line 32.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 229 |   565/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.08\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 230 |     1/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 36.81\n",
      "| epoch 230 |    95/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.25\n",
      "| epoch 230 |   189/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.55\n",
      "| epoch 230 |   283/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.46\n",
      "| epoch 230 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.23\n",
      "| epoch 230 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.31\n",
      "| epoch 230 |   565/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.30\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 231 |     1/  658 batches | total loss  1.14 | seq2seq loss  1.13 | style loss  0.00 | coherence loss  0.01| total accuracy  0.32 | total BLEU 34.71\n",
      "| epoch 231 |    95/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 40.41\n",
      "| epoch 231 |   189/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 40.44\n",
      "| epoch 231 |   283/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.56\n",
      "| epoch 231 |   377/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.44\n",
      "| epoch 231 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.34\n",
      "| epoch 231 |   565/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.26\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 232 |     1/  658 batches | total loss  0.96 | seq2seq loss  0.95 | style loss  0.00 | coherence loss  0.01| total accuracy  0.19 | total BLEU 43.44\n",
      "| epoch 232 |    95/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 40.31\n",
      "| epoch 232 |   189/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.39\n",
      "| epoch 232 |   283/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.20\n",
      "| epoch 232 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.18\n",
      "| epoch 232 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.14\n",
      "| epoch 232 |   565/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.13\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 233 |     1/  658 batches | total loss  1.21 | seq2seq loss  1.21 | style loss  0.00 | coherence loss  0.01| total accuracy  0.27 | total BLEU 38.53\n",
      "| epoch 233 |    95/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.82\n",
      "| epoch 233 |   189/  658 batches | total loss  1.08 | seq2seq loss  1.08 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.73\n",
      "| epoch 233 |   283/  658 batches | total loss  1.08 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 39.90\n",
      "| epoch 233 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.10\n",
      "| epoch 233 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.12\n",
      "| epoch 233 |   565/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.15\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 234 |     1/  658 batches | total loss  0.83 | seq2seq loss  0.82 | style loss  0.00 | coherence loss  0.01| total accuracy  0.31 | total BLEU 53.89\n",
      "| epoch 234 |    95/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.65\n",
      "| epoch 234 |   189/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.48\n",
      "| epoch 234 |   283/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.35\n",
      "| epoch 234 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.13\n",
      "| epoch 234 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.12\n",
      "| epoch 234 |   565/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-bleu.perl script returned non-zero exit code\n",
      "b'\\ngzip: /tmp/tmpw806xugz: not in gzip format\\nERROR: could not find reference file /tmp/tmpw806xugz at /tmp/tmp8ctmijiv line 32.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 235 |     1/  658 batches | total loss  1.01 | seq2seq loss  1.00 | style loss  0.00 | coherence loss  0.01| total accuracy  0.32 | total BLEU 43.96\n",
      "| epoch 235 |    95/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.91\n",
      "| epoch 235 |   189/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.19\n",
      "| epoch 235 |   283/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.20\n",
      "| epoch 235 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.10\n",
      "| epoch 235 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.08\n",
      "| epoch 235 |   565/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.13\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 236 |     1/  658 batches | total loss  1.31 | seq2seq loss  1.30 | style loss  0.00 | coherence loss  0.01| total accuracy  0.20 | total BLEU 31.80\n",
      "| epoch 236 |    95/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 40.70\n",
      "| epoch 236 |   189/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.38\n",
      "| epoch 236 |   283/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.29\n",
      "| epoch 236 |   377/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.44\n",
      "| epoch 236 |   471/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.50\n",
      "| epoch 236 |   565/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.39\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 237 |     1/  658 batches | total loss  0.84 | seq2seq loss  0.83 | style loss  0.00 | coherence loss  0.01| total accuracy  0.32 | total BLEU 43.93\n",
      "| epoch 237 |    95/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.47\n",
      "| epoch 237 |   189/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.95\n",
      "| epoch 237 |   283/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.09\n",
      "| epoch 237 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.22\n",
      "| epoch 237 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.17\n",
      "| epoch 237 |   565/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.31\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 238 |     1/  658 batches | total loss  0.91 | seq2seq loss  0.90 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 43.25\n",
      "| epoch 238 |    95/  658 batches | total loss  1.04 | seq2seq loss  1.03 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 40.77\n",
      "| epoch 238 |   189/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 40.27\n",
      "| epoch 238 |   283/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.34\n",
      "| epoch 238 |   377/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 40.43\n",
      "| epoch 238 |   471/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 40.38\n",
      "| epoch 238 |   565/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.38\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 239 |     1/  658 batches | total loss  1.12 | seq2seq loss  1.11 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 36.50\n",
      "| epoch 239 |    95/  658 batches | total loss  1.07 | seq2seq loss  1.07 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 39.77\n",
      "| epoch 239 |   189/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.08\n",
      "| epoch 239 |   283/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.11\n",
      "| epoch 239 |   377/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.19\n",
      "| epoch 239 |   471/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.22\n",
      "| epoch 239 |   565/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.26\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 240 |     1/  658 batches | total loss  0.89 | seq2seq loss  0.89 | style loss  0.00 | coherence loss  0.00| total accuracy  0.26 | total BLEU 47.02\n",
      "| epoch 240 |    95/  658 batches | total loss  1.07 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 39.74\n",
      "| epoch 240 |   189/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.32\n",
      "| epoch 240 |   283/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.15\n",
      "| epoch 240 |   377/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.35\n",
      "| epoch 240 |   471/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.32\n",
      "| epoch 240 |   565/  658 batches | total loss  1.06 | seq2seq loss  1.06 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.31\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 241 |     1/  658 batches | total loss  1.15 | seq2seq loss  1.14 | style loss  0.00 | coherence loss  0.01| total accuracy  0.25 | total BLEU 34.90\n",
      "| epoch 241 |    95/  658 batches | total loss  1.05 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.23 | total BLEU 40.75\n",
      "| epoch 241 |   189/  658 batches | total loss  1.05 | seq2seq loss  1.04 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.81\n",
      "| epoch 241 |   283/  658 batches | total loss  1.05 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.73\n",
      "| epoch 241 |   377/  658 batches | total loss  1.05 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.80\n",
      "| epoch 241 |   471/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.68\n",
      "| epoch 241 |   565/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.56\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "| epoch 242 |     1/  658 batches | total loss  1.22 | seq2seq loss  1.21 | style loss  0.00 | coherence loss  0.00| total accuracy  0.20 | total BLEU 30.18\n",
      "| epoch 242 |    95/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.69\n",
      "| epoch 242 |   189/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to fetch multi-bleu.perl script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 242 |   283/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.49\n",
      "| epoch 242 |   377/  658 batches | total loss  1.06 | seq2seq loss  1.05 | style loss  0.00 | coherence loss  0.01| total accuracy  0.24 | total BLEU 40.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to fetch multi-bleu.perl script\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9ed67aad974e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_token_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtotal_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mBLEU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_moses_multi_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypotheses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mtotal_BLEU\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mBLEU\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mBLEU\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torchnlp/metrics/bleu.py\u001b[0m in \u001b[0;36mget_moses_multi_bleu\u001b[0;34m(hypotheses, references, lowercase)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mbleu_cmd\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreference_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mbleu_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleu_cmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mbleu_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbleu_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mbleu_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"BLEU = (.+?),\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 395\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    924\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_epoch = 300\n",
    "writer = SummaryWriter(\"data/runs/parallel_model_v0\")\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                            shuffle=True,collate_fn=train_data.collate)\n",
    "n = len(train_data.x) // batch_size\n",
    "\n",
    "for epoch in range(201,nb_epoch+1):\n",
    "    total_loss = total_seq = total_style = total_coh = total_accuracy = total_BLEU = 0\n",
    "    i = 0\n",
    "    \n",
    "    for x,y, ctx_x, ctx_y_1,ctx_y_2, label_x,len_y in train_loader:\n",
    "        i += 1\n",
    "        if i==n:\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x,_ = model.forward(x,ctx_x,y,label_x) #Output still embedded\n",
    "        y_pred_dist = decoder_linear(x)[:,:-1,:]\n",
    "        \n",
    "        \n",
    "        # Seq2Seq Loss with Embedding\n",
    "        #y_hot = torch.nn.functional.one_hot(y).float()\n",
    "        #y_hot[y_hot==dict_size] = 0.\n",
    "        loss_seq = loss_seq2seq(y_pred_dist.reshape(-1,dict_size),y[:,1:].reshape(-1))\n",
    "        \n",
    "        # Style Loss\n",
    "        loss_sty,_ = style_classifier.forward(inputs_embeds=x,attention_mask=(y != dict_size).int(),labels=(1-label_x).to(device))\n",
    "        \n",
    "        \n",
    "        # Coherence Loss\n",
    "        ctx_y = torch.cat([embedding(ctx_y_1),x,embedding(ctx_y_2)],dim=1)\n",
    "        mask_coh = torch.cat([(ctx_y_1 != dict_size).int(),\n",
    "                              ((y != dict_size)*(y != 0)*(y != 1)).int(),\n",
    "                              (ctx_y_2 != dict_size).int()],dim=1).to(device)\n",
    "        loss_coh,_ = coherence_classifier(inputs_embeds=ctx_y,attention_mask=mask_coh,labels=torch.LongTensor([1]*batch_size).to(device))\n",
    "        \n",
    "        \n",
    "        # Total Loss\n",
    "        loss = l1 * loss_seq + l2 * loss_sty + l3 *  loss_coh\n",
    "\n",
    "        # Step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_seq += loss_seq.item()\n",
    "        total_style += loss_sty.item()\n",
    "        total_coh += loss_coh.item()\n",
    "        \n",
    "        pred = torch.argmax(softmax_layer(y_pred_dist),dim=2)\n",
    "        hypotheses = [code2string(item[:len_],dict_token,sos=True) for item,len_ in zip(pred,len_y)]\n",
    "        references = [code2string(item[:len_],dict_token) for item,len_ in zip(y,len_y)]\n",
    "        accuracy,_,_ = get_token_accuracy(y[:,1:],pred)\n",
    "        total_accuracy += accuracy\n",
    "        BLEU = get_moses_multi_bleu(hypotheses,references,lowercase=True)\n",
    "        total_BLEU += BLEU if BLEU else 0\n",
    "\n",
    "    #Vizualization and saving model\n",
    "        if ((i-1) % 94 == 0):\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'total loss {:5.2f} | seq2seq loss {:5.2f} | '\n",
    "                  'style loss {:5.2f} | coherence loss {:5.2f}'\n",
    "                  '| total accuracy {:5.2f} | total BLEU {:5.2f}'.format(\n",
    "                    epoch, i, n,total_loss/i,total_seq/i,total_style/i,total_coh/i,total_accuracy/i,total_BLEU/i))\n",
    "    print('-' * 110)\n",
    "    writer.add_scalar('train_loss',total_loss/(n-1),epoch)\n",
    "    writer.add_scalar('train_loss_seq',total_seq/(n-1),epoch)\n",
    "    writer.add_scalar('train_loss_style',total_style/(n-1),epoch)\n",
    "    writer.add_scalar('train_loss_coh',total_coh/(n-1),epoch)\n",
    "    writer.add_scalar('train_word_accuracy',total_accuracy/(n-1),epoch)\n",
    "    writer.add_scalar('train_BLEU',total_BLEU/(n-1),epoch)\n",
    "    if (epoch%5==0):\n",
    "        torch.save(model,\"data/models/parallel_model_v0_epoch_\"+str(epoch))\n",
    "        torch.save(decoder_linear,\"data/models/parallel_decoder_linear_v0_epoch_\"+str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction = 'sum' l_r = 5e-4\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "rango = np.arange(len(loss_graph))\n",
    "plt.figure(1,figsize=(16,16))\n",
    "plt.subplot(231)\n",
    "plt.plot(rango,loss_graph)\n",
    "plt.title(\"Total Loss\")\n",
    "plt.grid()\n",
    "plt.subplot(234)\n",
    "plt.plot(rango,loss_graph)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Total Loss Log\")\n",
    "plt.grid()\n",
    "plt.subplot(232)\n",
    "plt.plot(rango,loss_seq_list)\n",
    "plt.title(\"Seq Loss\")\n",
    "plt.grid()\n",
    "plt.subplot(235)\n",
    "plt.plot(rango,loss_seq_list)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Seq Loss Log\")\n",
    "plt.grid()\n",
    "plt.subplot(233)\n",
    "plt.plot(rango,loss_style_list)\n",
    "plt.title(\"Style Loss\")\n",
    "plt.grid()\n",
    "plt.subplot(236)\n",
    "plt.plot(rango,loss_style_list)\n",
    "plt.title(\"Style Loss Log\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,ctx_x,_,_,label_x,_ = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = string2code(\"my dear juliet . do you want to hang out with me ?\".split(\" \"),dict_words).to(device)\n",
    "ctx = string2code(\"hello .\".split(\" \"),dict_words).to(device)\n",
    "label_x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Style : Modern\n",
      "Original Phrase : <SOS> i’ve lurked in the shadows here to watch the downfall of my enemies . <EOS>\n",
      "Target phrase : <SOS> here in these confines slyly have i lurked to watch the waning of mine enemies . <EOS>\n",
      "Context_x : <SOS> let him thank me , who helped him get there . i’ll head to france soon . <EOS>\n"
     ]
    }
   ],
   "source": [
    "if(label_x == 1):\n",
    "    print(\"Original Style : Shakespearian\")\n",
    "else:\n",
    "    print(\"Original Style : Modern\")\n",
    "print(\"Original Phrase :\",code2string(x,dict_token))\n",
    "print(\"Target phrase :\",code2string(y,dict_token))\n",
    "print(\"Context_x :\",code2string(ctx_x,dict_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with: <SOS> \n",
      "Produced phrase: <SOS> i have heard any unseasonable the best to the rest . <EOS> \n"
     ]
    }
   ],
   "source": [
    "    #Generation de phrase \n",
    "with torch.no_grad():\n",
    "        h_t = model.translator(x.unsqueeze(0),ctx_x.unsqueeze(0),y.unsqueeze(0),torch.tensor([label_x]).unsqueeze(0)).to(device)\n",
    "        phrase = torch.tensor([[0]]).to(device)\n",
    "        print(\"Starting with: \",end='')\n",
    "        for p in phrase:\n",
    "            print(dict_token[p.item()],end=' ')\n",
    "        print(\"\")\n",
    "        i = 0\n",
    "        limit = 20\n",
    "        flag = False\n",
    "        while(not(flag) and i != limit):\n",
    "            #mask = model._generate_square_subsequent_mask(phrase.shape[1])\n",
    "            y_aux = model.embed_layer(phrase)\n",
    "            y_pos = model.positional_layer(y_aux)\n",
    "            y_pred = model.sentence_decoder(y_pos,h_t).transpose(0,1)\n",
    "            y_pred = decoder_linear(y_pred)\n",
    "            y_pred = torch.argmax(softmax_layer(y_pred),dim = 2)\n",
    "            phrase = torch.cat((phrase,y_pred[:,-1].reshape((1,1))),0)\n",
    "            i += 1\n",
    "            flag = (y_pred[:,-1].item() == 1)\n",
    "        print(\"Produced phrase: \",end='')\n",
    "        for p in phrase:\n",
    "            print(dict_token[p.item()],end=' ')\n",
    "        print(\"\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = string2code(\"i love killing people .\".split(\" \"),dict_words).to(device)\n",
    "ctx = string2code(\"i have to tell you something . this is my passion .\".split(\" \"),dict_words).to(device)\n",
    "label_x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS> this fake astonishment of yours is just like your other pranks . <EOS> ,',\n",
       " '<SOS> go to the church . <EOS> !',\n",
       " '<SOS> and yet i may yet converted but see you , but methinks i see to your eyes . other men’s . i <EOS> to',\n",
       " '<SOS> how , voltemand , what the the quick norway ? <EOS> .',\n",
       " '<SOS> i want to uncles here to welcome to . <EOS> .',\n",
       " '<SOS> macduff is missing , and your noble son . <EOS> .',\n",
       " '<SOS> let you favor to nope . <EOS> !',\n",
       " '<SOS> how can i i return from to earth , unless my husband send from from from heaven and leaving earth ? <EOS> ,',\n",
       " '<SOS> are you call me me ? too worship ? <EOS> ?',\n",
       " '<SOS> i gives , pray madam . <EOS> .',\n",
       " '<SOS> o , ominous ! <EOS> are',\n",
       " '<SOS> less in the knowledge and the grace is are scarce the the earth’s , thus and than to divine . <EOS> ,',\n",
       " '<SOS> no no else . . <EOS> lord',\n",
       " '<SOS> i highness , would to force me to tell you <EOS> .',\n",
       " '<SOS> this would breed from me occasions , and i can try that i know work more <EOS> my',\n",
       " '<SOS> i i know up to staying up late is to . late . <EOS> .',\n",
       " '<SOS> yet have laid room , six scotches more . <EOS> of',\n",
       " '<SOS> run , proclaim , , the streets . <EOS> never',\n",
       " '<SOS> if we’re faced on the job person we bruised , we’re we hardship to we try to say them to shush as to <EOS> than',\n",
       " '<SOS> this song’s about my father . . <EOS> .',\n",
       " '<SOS> is ’t possible ? <EOS> ?',\n",
       " '<SOS> we shall not both half of our affair . <EOS> did',\n",
       " '<SOS> o life ! <EOS> ?',\n",
       " '<SOS> iago’s , good man . <EOS> here',\n",
       " '<SOS> i would scarce trust myself to though i had sworn not contrary of if i would not me wife . <EOS> .',\n",
       " '<SOS> your honor calls a better you are leaving you <EOS> .',\n",
       " '<SOS> why are you acting like me like this ? <EOS> ?',\n",
       " '<SOS> would gentleness of all is gods . way you . <EOS> ,',\n",
       " '<SOS> oh , cassius , i’ll run far from this country he’ll where they’ll romans will find out . <EOS> else',\n",
       " '<SOS> he knew known when he , no music and the , the drum and the fife , and the , a rather hear the tabor and the pipe . <EOS>',\n",
       " '<SOS> close up , here , good fleance , run , run ! run , <EOS> !',\n",
       " '<SOS> a need a calendar ! <EOS> all']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS> this fake astonishment of yours is just like your other pranks . <EOS>',\n",
       " '<SOS> go to the church . <EOS>',\n",
       " '<SOS> and how you may be converted i know not , but methinks you look with your eyes as other women do . <EOS>',\n",
       " '<SOS> say , voltemand , what from our brother norway ? <EOS>',\n",
       " '<SOS> i want more uncles here to welcome me . <EOS>',\n",
       " '<SOS> macduff is missing , and your noble son . <EOS>',\n",
       " '<SOS> as a favor to nope . <EOS>',\n",
       " '<SOS> how shall that faith return again to earth , unless that husband send it me from heaven by leaving earth ? <EOS>',\n",
       " '<SOS> did you call for me , your worship ? <EOS>',\n",
       " '<SOS> that will i , pompey . <EOS>',\n",
       " '<SOS> oh , ominous ! <EOS>',\n",
       " '<SOS> less in your knowledge and your grace you show not than our earth’s wonder , more than earth divine . <EOS>',\n",
       " '<SOS> there’s none else by . <EOS>',\n",
       " '<SOS> your highness will have to force me to tell . <EOS>',\n",
       " '<SOS> i would breed from hence occasions , and i shall , that i may speak . <EOS>',\n",
       " '<SOS> all i know is that staying up late is staying up late . <EOS>',\n",
       " '<SOS> i have yet room for six scotches more . <EOS>',\n",
       " '<SOS> run and proclaim it in the streets . <EOS>',\n",
       " '<SOS> when we’re faced with a wretched person , bruised and crying with hardship , we try to get them to shush up . <EOS>',\n",
       " '<SOS> this song’s about my dead father . <EOS>',\n",
       " '<SOS> is ’t possible ? <EOS>',\n",
       " '<SOS> we have lost best half of our affair . <EOS>',\n",
       " '<SOS> o life ! <EOS>',\n",
       " '<SOS> iago’s a good man . <EOS>',\n",
       " '<SOS> i would scarce trust myself , though i had sworn the contrary , if hero would be my wife . <EOS>',\n",
       " '<SOS> your honor is the reason you are leaving . <EOS>',\n",
       " '<SOS> why are you talking to me like this ? <EOS>',\n",
       " '<SOS> the gentleness of all the gods go with thee ! <EOS>',\n",
       " '<SOS> oh , cassius , i’ll run far from this country to where no romans can find me . <EOS>',\n",
       " '<SOS> i have known when there was no music with him but the drum and the fife , and now had he rather hear the tabor and the pipe . <EOS>',\n",
       " '<SOS> get out of here , good fleance , run , run , run ! <EOS>',\n",
       " '<SOS> we need a calendar ! <EOS>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
