{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Parallel Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Python Notebook concerning the final version containing the coding for the Only-Parallel (Supervised) version of the program. It is to be completed with the correct format of the Contextual and Style Classifiers in their corresponding parts.\n",
    "\n",
    "Date of upload: Friday 31th January\n",
    "\n",
    "Actual Version: 1.1 (Friday 7th February)\n",
    "\n",
    "Precedent Versions : 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cpu\n"
     ]
    }
   ],
   "source": [
    "from scripts.data_builders.prepare_dataset import prepare_dataset,string2code,code2string\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device = \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ...\n",
      "- Shakespeare dataset length :  20316\n",
      "- Corrupted samples (ignored) :  763\n"
     ]
    }
   ],
   "source": [
    "train_data, dict_words = prepare_dataset(device,ratio=0.5,shuffle_ctx=False) #check with shift+tab to look at the data structure\n",
    "batch_size = 64\n",
    "dict_token = {b:a for a,b in dict_words.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_heads = 4\n",
    "d_feedforward = 1024\n",
    "batch_size = 64\n",
    "\n",
    "d_embedding = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Classifier (To be filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style classifier (To be filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (To be adapted once we have the Context and Style Class For the Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelModel(torch.nn.Module):\n",
    "    def __init__(self,dict_size, d_embedding,nb_heads, d_feedforward):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_layer = torch.nn.Embedding(dict_size+1, d_embedding, padding_idx=dict_size)\n",
    "        self.positional_layer = PositionalEncoding(d_embedding)\n",
    "        self.sentence_encoder = torch.nn.TransformerEncoderLayer(d_model = d_embedding, nhead = nb_heads, \n",
    "                                                    dim_feedforward = d_feedforward)\n",
    "        self.context_encoder = torch.nn.TransformerEncoderLayer(d_model = d_embedding, nhead = nb_heads, \n",
    "                                                    dim_feedforward = d_feedforward)\n",
    "        self.sentence_decoder = torch.nn.TransformerDecoderLayer(d_model = d_embedding + 1, nhead = nb_heads, \n",
    "                                                    dim_feedforward = d_feedforward)\n",
    "        self.label_embedding = torch.nn.Embedding(2,)\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(),\n",
    "            torch.nn.LogSoftmax(dim=2))\n",
    "    \n",
    "    \n",
    "    def _generate_square_subsequent_mask(sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self,x,ctx_x,y,label_y):\n",
    "        device = x.device\n",
    "        mask_x = self._generate_square_subsequent_mask(x.shape[1]).to(device)\n",
    "        mask_ctx = self._generate_square_subsequent_mask(ctx_x.shape[1]).to(device)\n",
    "        mask_y = self._generate_square_subsequent_mask(y.shape[1]).to(device)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embed_layer(x).transpose(0,1) # token x batch x embedding\n",
    "        ctx_x = self.embed_layer(ctx_layer).transpose(0,1)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        x = self.positional_layer(x)\n",
    "        ctx_x = self.positional_layer(x)\n",
    "        \n",
    "        # Encoders\n",
    "        x_enc = self.sentence_encoder(x,mask_x)\n",
    "        ctx_enc = self.context_encoder(x,mask_ctx)\n",
    "        \n",
    "        # Linear and Style Mixing\n",
    "        x_and_ctx = torch.cat((x_enc,ctx_enc),dim = 0)\n",
    "        label = label.reshape((1,label.shape[0])).expand((x_and_ctx.shape[0],x_and_ctx.shape[1]))\n",
    "        x_lab = x_and_ctx + self.label_embedding(label)\n",
    "        \n",
    "        # Decoder\n",
    "        y = embed_layer(y)\n",
    "        y_pos = self.positional_layer(y.transpose(0,1))\n",
    "        y_pred = sentence_decoder(y_pos,x_lab,mask_y)\n",
    "        \n",
    "        return(y_pred.transpose(0,1),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training  (To be adapted once we have the Context and Style Class For the Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the model(s)\n",
    "\n",
    "model = ParallelModel(dict_size, d_embedding, nb_heads, d_feedforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information concerning the Training optimizer\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "params = list(sentence_encoder.parameters()) + list(context_encoder.parameters()) \n",
    "        + list(linear_context.parameters) + list(sentence_decoder.parameters())\n",
    "\n",
    "l_r = 5e-4\n",
    "optimizer=Adam(params,lr=l_r)\n",
    "l1=1 #\n",
    "l2=1\n",
    "l3=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses \n",
    "loss_seq2seq = torch.nn.SmoothL1Loss() #Contextual Seq2Seq Loss\n",
    "loss_coherence = torch.nn.CrossEntropyLoss() #Contextual Coherence Loss\n",
    "loss_style = torch.nn.BCELoss() #Style Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "for epoch in range(nb_epoch):\n",
    "    for x, y, ctx_x,ctx_y , len_x,len_y , len_ctx_x,len_ctx_y, label,label_ctx in train_loader:\n",
    "        y_pred, y = model.forward(x,ctx_x,y,label) #Output still embedded \n",
    "        \n",
    "        # Seq2Seq Loss with Token\n",
    "        \n",
    "        loss_seq = l1*loss_seq2seq(y_pred,y)\n",
    "        \n",
    "        # Argmax\n",
    "        y_pred = torch.argmax(softmax_layer(LogSoftmax),dim = 2)\n",
    "        \n",
    "        # Context + Style class\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
