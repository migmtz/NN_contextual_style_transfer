{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Parallel Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Python Notebook concerning the final version containing the coding for the Only-Parallel (Supervised) version of the program. It is to be completed with the correct format of the Contextual and Style Classifiers in their corresponding parts.\n",
    "\n",
    "Date of upload: Friday 31th January\n",
    "\n",
    "Actual Version: 2.1, translation implemented (Monday 10th February)\n",
    "\n",
    "Precedent Versions : 2.0, 1.1, 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cpu\n"
     ]
    }
   ],
   "source": [
    "from scripts.data_builders.prepare_dataset import prepare_dataset,string2code,code2string\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device = \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ...\n",
      "- Shakespeare dataset length :  21079\n",
      "- Corrupted samples (ignored) :  0\n"
     ]
    }
   ],
   "source": [
    "train_data, dict_words = prepare_dataset(\"data/shakespeare.csv\",device,ratio=0.5,shuffle_ctx=False) #check with shift+tab to look at the data structure\n",
    "batch_size = 64\n",
    "dict_token = {b:a for a,b in dict_words.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self,model,optim):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.epoch = 0\n",
    "savepath = Path(\"data/models/embeddings/v0\")\n",
    "embedding = torch.load(savepath).to(device)\n",
    "embedding.weight.requires_grad = False #Embedding freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_heads = 4\n",
    "d_feedforward = 1024\n",
    "batch_size = 64\n",
    "dict_size = len(dict_token)\n",
    "\n",
    "d_embedding = embedding.embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Classifier (To be filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style classifier (To be filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose(torch.nn.Module):\n",
    "    def __init__(self,dim0,dim1):\n",
    "        super().__init__()\n",
    "        self.dim0=dim0\n",
    "        self.dim1=dim1\n",
    "\n",
    "    def forward(self,x):\n",
    "        return torch.transpose(x,dim0=self.dim0,dim1=self.dim1)\n",
    "\n",
    "class Reduce(torch.nn.Module):\n",
    "    def __init__(self,dim):\n",
    "        super().__init__()\n",
    "        self.dim=dim\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=torch.sum(x,dim=self.dim)\n",
    "        return x.reshape(x.shape[0],x.shape[1])\n",
    "\n",
    "class StyleClassifier(torch.nn.Module):\n",
    "    def __init__(self,dict_size=18000,d_embedding=300,d_hidden=10):\n",
    "        super(StyleClassifier,self).__init__()\n",
    "        self.embedding=nn.Embedding(dict_size+1,d_embedding,padding_idx=dict_size)\n",
    "        self.transpose=Transpose(1,2)\n",
    "        self.conv1d=nn.Conv1d(d_embedding,d_hidden,3,1)\n",
    "        self.reduce=Reduce(2)\n",
    "        self.pool=nn.MaxPool1d(3,1)\n",
    "        self.linear=nn.Linear(d_hidden,1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.model=nn.Sequential(\n",
    "            self.embedding,\n",
    "            self.transpose,\n",
    "            self.conv1d,\n",
    "            self.pool,\n",
    "            self.reduce,\n",
    "            self.linear,\n",
    "            self.sigmoid)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "        \n",
    "        \n",
    "savepath_style = Path(\"data/models/style_classifier_v0\")\n",
    "style_classifier = torch.load(savepath_style).model.to(device)\n",
    "style_classifier.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (To be adapted once we have the Context and Style Class For the Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelModel(torch.nn.Module):\n",
    "    def __init__(self,dict_size, d_embedding, nb_heads, d_feedforward):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_layer = torch.nn.Embedding(dict_size+1, d_embedding, padding_idx=dict_size)\n",
    "        self.positional_layer = PositionalEncoding(d_embedding)\n",
    "        self.sentence_encoder = torch.nn.TransformerEncoderLayer(d_model = d_embedding, nhead = nb_heads, \n",
    "                                                    dim_feedforward = d_feedforward)\n",
    "        self.context_encoder = torch.nn.TransformerEncoderLayer(d_model = d_embedding, nhead = nb_heads, \n",
    "                                                    dim_feedforward = d_feedforward)\n",
    "        self.sentence_decoder = torch.nn.TransformerDecoderLayer(d_model = d_embedding, nhead = nb_heads, \n",
    "                                                    dim_feedforward = d_feedforward)\n",
    "        self.label_embedding = torch.nn.Embedding(2,768)\n",
    "    \n",
    "    \n",
    "    def _generate_square_subsequent_mask(self,sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self,x,ctx_x,y,label_x):\n",
    "        device = x.device\n",
    "        mask_x = self._generate_square_subsequent_mask(x.shape[1]).to(device)\n",
    "        mask_ctx = self._generate_square_subsequent_mask(ctx_x.shape[1]).to(device)\n",
    "        mask_y = self._generate_square_subsequent_mask(y.shape[1]).to(device)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embed_layer(x).transpose(0,1) # token x batch x embedding\n",
    "        ctx_x = self.embed_layer(ctx_x).transpose(0,1)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        x = self.positional_layer(x)\n",
    "        ctx_x = self.positional_layer(ctx_x)        \n",
    "        \n",
    "        # Encoders\n",
    "        x_enc = self.sentence_encoder(x,mask_x)\n",
    "        ctx_enc = self.context_encoder(ctx_x,mask_ctx)\n",
    "\n",
    "        # Linear and Style Mixing\n",
    "        x_and_ctx = torch.cat((x_enc,ctx_enc),dim = 0)\n",
    "        label = (1-label_x).reshape((1,x_and_ctx.shape[1])).expand((x_and_ctx.shape[0],x_and_ctx.shape[1]))\n",
    "        x_lab = x_and_ctx + self.label_embedding(label)\n",
    "        \n",
    "        # Decoder\n",
    "        y = self.embed_layer(y)\n",
    "        y_pos = self.positional_layer(y.transpose(0,1))\n",
    "        y_pred = self.sentence_decoder(y_pos,x_lab,mask_y)\n",
    "        \n",
    "        return(y_pred.transpose(0,1),y)\n",
    "    \n",
    "    def translator(self,x,ctx_x,y,label_x):\n",
    "        device = x.device\n",
    "        mask_x = self._generate_square_subsequent_mask(x.shape[1]).to(device)\n",
    "        mask_ctx = self._generate_square_subsequent_mask(ctx_x.shape[1]).to(device)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embed_layer(x).transpose(0,1) # token x batch x embedding\n",
    "        ctx_x = self.embed_layer(ctx_x).transpose(0,1)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        x = self.positional_layer(x)\n",
    "        ctx_x = self.positional_layer(ctx_x)        \n",
    "        \n",
    "        # Encoders\n",
    "        x_enc = self.sentence_encoder(x,mask_x)\n",
    "        ctx_enc = self.context_encoder(ctx_x,mask_ctx)\n",
    "        \n",
    "        # Linear and Style Mixing\n",
    "        x_and_ctx = torch.cat((x_enc,ctx_enc),dim = 0)\n",
    "        label = (1-label_x).reshape((1,x_and_ctx.shape[1])).expand((x_and_ctx.shape[0],x_and_ctx.shape[1]))\n",
    "        x_lab = x_and_ctx + self.label_embedding(label)\n",
    "        return(x_lab)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training  (To be adapted once we have the Context and Style Class For the Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the model(s)\n",
    "\n",
    "model = ParallelModel(dict_size, d_embedding, nb_heads, d_feedforward).to(device)\n",
    "model.embed_layer = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information concerning the Training optimizer\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "decoder_linear = torch.nn.Linear(d_embedding,dict_size)\n",
    "\n",
    "softmax_layer = torch.nn.LogSoftmax(dim = 2)\n",
    "\n",
    "params = list(model.parameters()) + list(decoder_linear.parameters())#+ list(context_encoder.parameters()) + \n",
    "                                  #list(linear_context.parameters) + list(sentence_decoder.parameters())\n",
    "\n",
    "l_r = 5e-2\n",
    "optimizer=Adam(params,lr=l_r)\n",
    "l1=1 #\n",
    "l2=1e-2\n",
    "l3=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses \n",
    "loss_seq2seq = torch.nn.SmoothL1Loss(reduction='mean') #Contextual Seq2Seq Loss\n",
    "loss_coherence = torch.nn.CrossEntropyLoss() #Contextual Coherence Loss\n",
    "loss_style = torch.nn.BCELoss(reduction = 'mean') #Style Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     1/  330 batches | loss 612395.56 | loss_seq  611837.12 | loss_style 558.42 | total loss 612395.56 | l_rate 1e-02\n",
      "| epoch   1 |     5/  330 batches | loss 1363752.25 | loss_seq  1363372.12 | loss_style 380.14 | total loss 1055496.60 | l_rate 1e-02\n",
      "| epoch   1 |    10/  330 batches | loss 744846.12 | loss_seq  744250.12 | loss_style 595.99 | total loss 936062.73 | l_rate 1e-02\n",
      "| epoch   1 |    15/  330 batches | loss 600796.31 | loss_seq  600042.06 | loss_style 754.23 | total loss 890606.48 | l_rate 1e-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-fde936c8dd8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Argmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_epoch = 1\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                            shuffle=True,collate_fn=train_data.collate)\n",
    "n = len(train_data.x) // batch_size\n",
    "loss_graph = []\n",
    "loss_style_list = []\n",
    "loss_seq_list = []\n",
    "for epoch in range(nb_epoch):\n",
    "    total_loss = 0\n",
    "    total_style = 0\n",
    "    total_seq = 0\n",
    "    i = 0\n",
    "    for x, y, ctx_x,ctx_y , len_x,len_y , len_ctx_x,len_ctx_y, label,label_ctx in train_loader:\n",
    "        i += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred, y = model.forward(x,ctx_x,y,label) #Output still embedded \n",
    "        \n",
    "        # Seq2Seq Loss with Token\n",
    "        \n",
    "        loss_seq = l1*loss_seq2seq(y_pred,y)\n",
    "        \n",
    "        # Argmax\n",
    "        y_pred = decoder_linear(y_pred)\n",
    "        y_pred = torch.argmax(softmax_layer(y_pred),dim = 2)\n",
    "        \n",
    "        # Style class\n",
    "        style_pred = style_classifier.forward(y_pred)\n",
    "        loss_sty = l2*loss_style(style_pred, (1-label).float())\n",
    "        \n",
    "        # Step\n",
    "        \n",
    "        loss = loss_seq + loss_sty\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_style += loss_sty.item()\n",
    "        total_seq += loss_seq.item()\n",
    "        if(i == 1 or i%5 == 0 or i == n):\n",
    "          print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                'loss {:5.2f} | loss_seq  {:5.2f} | loss_style {:5.2f} | '\n",
    "                'total loss {:5.2f} | l_rate {:.0e}'.format(\n",
    "                  epoch+1, i, n+1,loss.item(),loss_seq.item(),loss_sty.item(),\n",
    "                    total_loss/i,optimizer.param_groups[0][\"lr\"]))\n",
    "        loss_graph += [total_loss/i]\n",
    "        loss_style_list += [total_style/i]\n",
    "        loss_seq_list += [total_seq/i]\n",
    "    print('-' * 70)\n",
    "    print(\"Epoch \",epoch+1,\"\\t\",round(total_loss / n,2))\n",
    "    print('-' * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction = 'sum' l_r = 5e-4\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "rango = np.arange(len(loss_graph))\n",
    "plt.figure(1,figsize=(16,16))\n",
    "plt.subplot(231)\n",
    "plt.plot(rango,loss_graph)\n",
    "plt.title(\"Total Loss\")\n",
    "plt.grid()\n",
    "plt.subplot(234)\n",
    "plt.plot(rango,loss_graph)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Total Loss Log\")\n",
    "plt.grid()\n",
    "plt.subplot(232)\n",
    "plt.plot(rango,loss_seq_list)\n",
    "plt.title(\"Seq Loss\")\n",
    "plt.grid()\n",
    "plt.subplot(235)\n",
    "plt.plot(rango,loss_seq_list)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Seq Loss Log\")\n",
    "plt.grid()\n",
    "plt.subplot(233)\n",
    "plt.plot(rango,loss_style_list)\n",
    "plt.title(\"Style Loss\")\n",
    "plt.grid()\n",
    "plt.subplot(236)\n",
    "plt.plot(rango,loss_style_list)\n",
    "plt.title(\"Style Loss Log\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Phrase I HAVE A MIND TO STRIKE THEE ERE\n",
      "Target phrase I HAVE HALF A MIND TO HIT YOU BEFORE YOU SPEAK\n",
      "Original Style : Shakespearian\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y,ctx_x,_,_,_,_,_,label_x,_ = train_data[0]\n",
    "print(\"Original Phrase\",code2string(x,dict_token))\n",
    "print(\"Target phrase\",code2string(y,dict_token))\n",
    "if(label_x.item() == 1):\n",
    "    print(\"Original Style : Shakespearian\")\n",
    "else:\n",
    "    print(\"Original Style : Modern\")\n",
    "x.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with: <SOS> \n",
      "torch.Size([1, 1])\n",
      "tensor([15183])\n",
      "Produced phrase: <SOS> AMPLE \n",
      "torch.Size([1, 2])\n",
      "tensor([15183])\n",
      "Produced phrase: <SOS> AMPLE AMPLE \n",
      "torch.Size([1, 3])\n",
      "tensor([15183])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE \n",
      "torch.Size([1, 4])\n",
      "tensor([15241])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY \n",
      "torch.Size([1, 5])\n",
      "tensor([10566])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE \n",
      "torch.Size([1, 6])\n",
      "tensor([15183])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE \n",
      "torch.Size([1, 7])\n",
      "tensor([10566])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE \n",
      "torch.Size([1, 8])\n",
      "tensor([10566])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE DO-DE \n",
      "torch.Size([1, 9])\n",
      "tensor([15241])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE DO-DE BERKELEY \n",
      "torch.Size([1, 10])\n",
      "tensor([64])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE DO-DE BERKELEY THAN \n",
      "torch.Size([1, 11])\n",
      "tensor([10566])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE DO-DE BERKELEY THAN DO-DE \n",
      "torch.Size([1, 12])\n",
      "tensor([12599])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE DO-DE BERKELEY THAN DO-DE NOTARY \n",
      "torch.Size([1, 13])\n",
      "tensor([15183])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE DO-DE BERKELEY THAN DO-DE NOTARY AMPLE \n",
      "torch.Size([1, 14])\n",
      "tensor([15241])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE DO-DE BERKELEY THAN DO-DE NOTARY AMPLE BERKELEY \n",
      "torch.Size([1, 15])\n",
      "tensor([4536])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE DO-DE BERKELEY THAN DO-DE NOTARY AMPLE BERKELEY ESTATE \n",
      "torch.Size([1, 16])\n",
      "tensor([10566])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE DO-DE BERKELEY THAN DO-DE NOTARY AMPLE BERKELEY ESTATE DO-DE \n",
      "torch.Size([1, 17])\n",
      "tensor([15241])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE DO-DE BERKELEY THAN DO-DE NOTARY AMPLE BERKELEY ESTATE DO-DE BERKELEY \n",
      "torch.Size([1, 18])\n",
      "tensor([10566])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE DO-DE BERKELEY THAN DO-DE NOTARY AMPLE BERKELEY ESTATE DO-DE BERKELEY DO-DE \n",
      "torch.Size([1, 19])\n",
      "tensor([10566])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE DO-DE BERKELEY THAN DO-DE NOTARY AMPLE BERKELEY ESTATE DO-DE BERKELEY DO-DE DO-DE \n",
      "torch.Size([1, 20])\n",
      "tensor([10566])\n",
      "Produced phrase: <SOS> AMPLE AMPLE AMPLE BERKELEY DO-DE AMPLE DO-DE DO-DE BERKELEY THAN DO-DE NOTARY AMPLE BERKELEY ESTATE DO-DE BERKELEY DO-DE DO-DE DO-DE \n"
     ]
    }
   ],
   "source": [
    "    #Generation de phrase \n",
    "with torch.no_grad():\n",
    "        h_t = model.translator(x.unsqueeze(0),ctx_x.unsqueeze(0),y.unsqueeze(0),label_x.unsqueeze(0))\n",
    "        phrase = torch.tensor([[0]])\n",
    "        print(\"Starting with: \",end='')\n",
    "        for p in phrase:\n",
    "            print(dict_token[p.item()],end=' ')\n",
    "        print(\"\")\n",
    "        i = 0\n",
    "        limit = 20\n",
    "        flag = False\n",
    "        while(not(flag) and i != limit):\n",
    "            #mask = model._generate_square_subsequent_mask(phrase.shape[1])\n",
    "            y_aux = model.embed_layer(phrase)\n",
    "            y_pos = model.positional_layer(y_aux)\n",
    "            y_pred = model.sentence_decoder(y_pos,h_t).transpose(0,1)\n",
    "            y_pred = decoder_linear(y_pred)\n",
    "            y_pred = torch.argmax(softmax_layer(y_pred),dim = 2)\n",
    "            phrase = torch.cat((phrase,y_pred[:,-1].reshape((1,1))),0)\n",
    "            i += 1\n",
    "        print(\"Produced phrase: \",end='')\n",
    "        for p in phrase:\n",
    "            print(dict_token[p.item()],end=' ')\n",
    "        print(\"\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
