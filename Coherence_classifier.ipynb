{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>PROJECT SANDBOX</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "The aim of this notebook is to provide a simple sandbox to test different NN architectures for the project. , here is a doc about the functions imported from `scripts` folder : \n",
    "\n",
    "- **`prepare_dataset(device,ratio=0.5,shuffle_ctx=False)`** :\n",
    "    - **Input**:\n",
    "        - device : a torch.device object\n",
    "        - ratio : a float ratio between 0 and 1 that determines the average proportion of modern english verses in the data loader\n",
    "        - shuffle_ctx : if `True`, shuffle the contexts within a Batch so that half of the `x_1` elements has a wrong context `ctx_1`. Useful to train the context recognizer model.\n",
    "    - **Return** :\n",
    "        - a torch Dataset | class : Shakespeare inherited from torch.utils.data.Dataset\n",
    "        - a python word dictionary (aka tokenizer) | class : dict\n",
    "    - **Tensors returned when loaded in the dataloader**:\n",
    "        - x_1 : input verse (modern / shakespearian)\n",
    "        - x_2 : output verse (modern / shakespearian)\n",
    "\n",
    "        - ctx_1 = context of the input verse\n",
    "        - ctx_2 = context of the output verse\n",
    "\n",
    "        - len_x : length of the input verse\n",
    "        - len_y : length of the output verse\n",
    "\n",
    "        - len_ctx_x : length of the input verse context\n",
    "        - len_ctx_y : length of the output verse context\n",
    "\n",
    "        - label : label of the input verse (0 : modern, 1 : shakespearian)\n",
    "        - label_ctx : label of the context (0 : wrong context, 1 : right context)\n",
    "- **`string2code(string,dict)`** : \n",
    "    - **Input**:\n",
    "        - string : a sentence\n",
    "        - dict : a tokenizer\n",
    "    - **Return** :\n",
    "        - a torch Longtensor (sentence tokenized)\n",
    "- **`code2string(torch.Longtensor,dict)`** : \n",
    "    - **Input**:\n",
    "        - torch.Longtensor : a sentence tokenized\n",
    "        - dict : a tokenizer\n",
    "    - **Return** :\n",
    "        - a string sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "from scripts.data_builders.prepare_dataset import prepare_dataset_ctx,string2code,code2string,assemble\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import math\n",
    "from torch.nn import BCELoss,CrossEntropyLoss\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import pickle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device = \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ...\n",
      "- Shakespeare context dataset length :  21079\n",
      "- Corrupted samples (ignored) :  0\n",
      "- dict size :  17513\n"
     ]
    }
   ],
   "source": [
    "train_data, dict_words = prepare_dataset_ctx(device,ratio=0.5,shuffle_ctx=True) #check with shift+tab to look at the data structure\n",
    "batch_size = 16\n",
    "dict_token = {b:a for a,b in dict_words.items()} #dict for code2string\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                           shuffle=True,collate_fn=train_data.collate)\n",
    "dict_size = len(dict_words)\n",
    "d_embedding = 300 #cf. paper Y.Kim 2014 Convolutional Neural Networks for Sentence Classification\n",
    "\n",
    "print(\"- dict size : \",dict_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 : CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoherenceClassifier(torch.nn.Module):\n",
    "    def __init__(self,dict_size=dict_size,d_embedding=300):\n",
    "        super().__init__()\n",
    "        self.embed_layer=torch.nn.Embedding(dict_size+1,d_embedding,padding_idx=dict_size)\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv1d(d_embedding,3,kernel_size = 3, stride = 1)\n",
    "        self.max_pool = torch.nn.MaxPool1d(3,2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear = torch.nn.Linear(3,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.embed_layer(x)\n",
    "        x = self.conv_1( x.transpose(1,2))\n",
    "        x = self.max_pool( x )\n",
    "        x = self.relu( x )\n",
    "        x = torch.max( x , 2 )[0]\n",
    "        x = torch.sigmoid(self.linear(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "model = CoherenceClassifier().to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(),lr=0.01)\n",
    "loss_func = BCELoss()\n",
    "n = len(train_data.x) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 10, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-83eb7048ae67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlen_ctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_ctx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 10, got 4)"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    for _,_ , ctx,_ , _,_ , len_ctx,_, _,label_ctx in train_loader:\n",
    "        i+=1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #CNN\n",
    "        ctx = model.forward(ctx).reshape(-1) #CNN architecture\n",
    "        \n",
    "        loss = loss_func( ctx , label_ctx.float() )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        #Vizualization\n",
    "\n",
    "    print('-' * 35)\n",
    "    print('| epoch {:3d} | '\n",
    "          'lr {:02.2f} | '\n",
    "          'loss {:5.2f}'.format(\n",
    "            epoch+1, optimizer.state_dict()[\"param_groups\"][0][\"lr\"],\n",
    "            round(total_loss,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 : LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoherenceClassifier(torch.nn.Module):\n",
    "    def __init__(self,dict_size=dict_size,d_embedding=300,d_hidden=100):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.embedding = nn.Embedding(dict_size+1,d_embedding,padding_idx=dict_size)\n",
    "        self.lstm = nn.LSTM(d_embedding,self.d_hidden,dropout=0.,num_layers=1,bidirectional=False)\n",
    "        #self.bn0 = nn.BatchNorm1d(self.d_hidden)\n",
    "        self.linear1 = torch.nn.Linear(self.d_hidden,1)\n",
    "    \n",
    "    def forward(self,x,len_x):\n",
    "        x = self.embedding(x)\n",
    "        x = pack_padded_sequence(x.permute(1,0,2),len_x,enforce_sorted=False)\n",
    "        _,x = self.lstm(x)\n",
    "        x = x[0].reshape(-1,self.d_hidden)\n",
    "        #x = self.bn0(x)\n",
    "        x = torch.sigmoid( self.linear1(x) ).reshape(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "model = CoherenceClassifier().to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(),lr=0.01)\n",
    "loss_func = BCELoss()\n",
    "n = len(train_data.x) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    for _,_ , ctx,_ , _,_ , len_ctx,_, _,label_ctx in train_loader:\n",
    "        i+=1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #LSTM\n",
    "        ctx = model.forward(ctx,len_ctx) #LSTM architecture\n",
    "        \n",
    "        loss = loss_func( ctx , label_ctx.float() )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print('-' * 35)\n",
    "    print('| epoch {:3d} | '\n",
    "          'lr {:02.2f} | '\n",
    "          'loss {:5.2f}'.format(\n",
    "            epoch+1, optimizer.state_dict()[\"param_groups\"][0][\"lr\"],\n",
    "            round(total_loss,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 : Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoherenceClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self,dict_size=dict_size, d_embedding=300,  dropout=0.1):\n",
    "        super(CoherenceClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(dict_size+1,d_embedding,padding_idx=dict_size)\n",
    "        self.pos_encoder = PositionalEncoding(d_embedding, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model=d_embedding, nhead = 4,dropout=dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=4)\n",
    "        \n",
    "        self.decoder = nn.Linear(d_embedding, 2 )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedding( x )\n",
    "        #x = self.pos_encoder( x )\n",
    "        x = self.transformer_encoder( x )\n",
    "        x = torch.softmax(torch.tanh(self.decoder( x )),1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=50):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "model = CoherenceClassifier().to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(),lr=0.01)\n",
    "loss_func = CrossEntropyLoss()\n",
    "n = len(train_data.x) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    for _,_ , ctx,_ , _,_ , len_ctx,_, _,label_ctx in train_loader:\n",
    "        i+=1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Transformer\n",
    "        ctx = model.forward(ctx) #Transformer architecture\n",
    "        y = torch.cat([label_ctx.reshape(-1,1),1-label_ctx.reshape(-1,1)],dim=1) # Transformer architecture\n",
    "        \n",
    "        loss = loss_func( ctx , y )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print('-' * 35)\n",
    "    print('| epoch {:3d} | '\n",
    "          'lr {:02.2f} | '\n",
    "          'loss {:5.2f}'.format(\n",
    "            epoch+1, optimizer.state_dict()[\"param_groups\"][0][\"lr\"],\n",
    "            round(total_loss,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jb/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "#Load model and plug our Embedding in\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', 'bert-base-uncased').to(device)\n",
    "model.bert.embeddings.word_embeddings = nn.Embedding(dict_size+1,768,padding_idx=dict_size).to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(),lr=0.01)\n",
    "loss_func = CrossEntropyLoss()\n",
    "n = len(train_data.x) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     1/ 1317 batches | loss  0.65 |\n",
      "| epoch   1 |     2/ 1317 batches | loss  1.70 |\n",
      "| epoch   1 |     3/ 1317 batches | loss  4.12 |\n",
      "| epoch   1 |     4/ 1317 batches | loss  3.60 |\n",
      "| epoch   1 |     5/ 1317 batches | loss  0.70 |\n",
      "| epoch   1 |     6/ 1317 batches | loss  0.76 |\n",
      "| epoch   1 |     7/ 1317 batches | loss  6.59 |\n",
      "| epoch   1 |     8/ 1317 batches | loss  2.42 |\n",
      "| epoch   1 |     9/ 1317 batches | loss  1.08 |\n",
      "| epoch   1 |    10/ 1317 batches | loss  2.13 |\n",
      "| epoch   1 |    11/ 1317 batches | loss  0.68 |\n",
      "| epoch   1 |    12/ 1317 batches | loss  0.63 |\n",
      "| epoch   1 |    13/ 1317 batches | loss  1.78 |\n",
      "| epoch   1 |    14/ 1317 batches | loss  0.64 |\n",
      "| epoch   1 |    15/ 1317 batches | loss  0.77 |\n",
      "| epoch   1 |    16/ 1317 batches | loss  1.09 |\n",
      "| epoch   1 |    17/ 1317 batches | loss  0.56 |\n",
      "| epoch   1 |    18/ 1317 batches | loss  0.82 |\n",
      "| epoch   1 |    19/ 1317 batches | loss  0.62 |\n",
      "| epoch   1 |    20/ 1317 batches | loss  0.57 |\n",
      "| epoch   1 |    21/ 1317 batches | loss  0.96 |\n",
      "| epoch   1 |    22/ 1317 batches | loss  0.73 |\n",
      "| epoch   1 |    23/ 1317 batches | loss  0.78 |\n",
      "| epoch   1 |    24/ 1317 batches | loss  0.87 |\n",
      "| epoch   1 |    25/ 1317 batches | loss  0.70 |\n",
      "| epoch   1 |    26/ 1317 batches | loss  1.25 |\n",
      "| epoch   1 |    27/ 1317 batches | loss  1.18 |\n",
      "| epoch   1 |    28/ 1317 batches | loss  0.80 |\n",
      "| epoch   1 |    29/ 1317 batches | loss  1.71 |\n",
      "| epoch   1 |    30/ 1317 batches | loss  1.54 |\n",
      "| epoch   1 |    31/ 1317 batches | loss  0.51 |\n",
      "| epoch   1 |    32/ 1317 batches | loss  3.24 |\n",
      "| epoch   1 |    33/ 1317 batches | loss  1.74 |\n",
      "| epoch   1 |    34/ 1317 batches | loss  0.87 |\n",
      "| epoch   1 |    35/ 1317 batches | loss  1.81 |\n",
      "| epoch   1 |    36/ 1317 batches | loss  2.88 |\n",
      "| epoch   1 |    37/ 1317 batches | loss  0.87 |\n",
      "| epoch   1 |    38/ 1317 batches | loss  0.96 |\n",
      "| epoch   1 |    39/ 1317 batches | loss  3.57 |\n",
      "| epoch   1 |    40/ 1317 batches | loss  1.84 |\n",
      "| epoch   1 |    41/ 1317 batches | loss  0.64 |\n",
      "| epoch   1 |    42/ 1317 batches | loss  1.08 |\n",
      "| epoch   1 |    43/ 1317 batches | loss  2.89 |\n",
      "| epoch   1 |    44/ 1317 batches | loss  1.34 |\n",
      "| epoch   1 |    45/ 1317 batches | loss  0.74 |\n",
      "| epoch   1 |    46/ 1317 batches | loss  1.70 |\n",
      "| epoch   1 |    47/ 1317 batches | loss  2.09 |\n",
      "| epoch   1 |    48/ 1317 batches | loss  2.19 |\n",
      "| epoch   1 |    49/ 1317 batches | loss  1.51 |\n",
      "| epoch   1 |    50/ 1317 batches | loss  1.26 |\n",
      "| epoch   1 |    51/ 1317 batches | loss  2.59 |\n",
      "| epoch   1 |    52/ 1317 batches | loss  2.66 |\n",
      "| epoch   1 |    53/ 1317 batches | loss  1.54 |\n",
      "| epoch   1 |    54/ 1317 batches | loss  1.96 |\n",
      "| epoch   1 |    55/ 1317 batches | loss  1.81 |\n",
      "| epoch   1 |    56/ 1317 batches | loss  1.53 |\n",
      "| epoch   1 |    57/ 1317 batches | loss  0.74 |\n",
      "| epoch   1 |    58/ 1317 batches | loss  0.70 |\n",
      "| epoch   1 |    59/ 1317 batches | loss  2.00 |\n",
      "| epoch   1 |    60/ 1317 batches | loss  1.47 |\n",
      "| epoch   1 |    61/ 1317 batches | loss  0.76 |\n",
      "| epoch   1 |    62/ 1317 batches | loss  0.79 |\n",
      "| epoch   1 |    63/ 1317 batches | loss  1.02 |\n",
      "| epoch   1 |    64/ 1317 batches | loss  1.32 |\n",
      "| epoch   1 |    65/ 1317 batches | loss  0.70 |\n",
      "| epoch   1 |    66/ 1317 batches | loss  1.20 |\n",
      "| epoch   1 |    67/ 1317 batches | loss  0.98 |\n",
      "| epoch   1 |    68/ 1317 batches | loss  0.69 |\n",
      "| epoch   1 |    69/ 1317 batches | loss  0.91 |\n",
      "| epoch   1 |    70/ 1317 batches | loss  1.46 |\n",
      "| epoch   1 |    71/ 1317 batches | loss  0.95 |\n",
      "| epoch   1 |    72/ 1317 batches | loss  1.08 |\n",
      "| epoch   1 |    73/ 1317 batches | loss  1.03 |\n",
      "| epoch   1 |    74/ 1317 batches | loss  0.66 |\n",
      "| epoch   1 |    75/ 1317 batches | loss  0.79 |\n",
      "| epoch   1 |    76/ 1317 batches | loss  0.99 |\n",
      "| epoch   1 |    77/ 1317 batches | loss  0.52 |\n",
      "| epoch   1 |    78/ 1317 batches | loss  0.83 |\n",
      "| epoch   1 |    79/ 1317 batches | loss  0.72 |\n",
      "| epoch   1 |    80/ 1317 batches | loss  0.77 |\n",
      "| epoch   1 |    81/ 1317 batches | loss  0.87 |\n",
      "| epoch   1 |    82/ 1317 batches | loss  0.73 |\n",
      "| epoch   1 |    83/ 1317 batches | loss  0.81 |\n",
      "| epoch   1 |    84/ 1317 batches | loss  0.61 |\n",
      "| epoch   1 |    85/ 1317 batches | loss  0.34 |\n",
      "| epoch   1 |    86/ 1317 batches | loss  0.89 |\n",
      "| epoch   1 |    87/ 1317 batches | loss  0.98 |\n",
      "| epoch   1 |    88/ 1317 batches | loss  0.86 |\n",
      "| epoch   1 |    89/ 1317 batches | loss  0.76 |\n",
      "| epoch   1 |    90/ 1317 batches | loss  1.59 |\n",
      "| epoch   1 |    91/ 1317 batches | loss  1.23 |\n",
      "| epoch   1 |    92/ 1317 batches | loss  0.70 |\n",
      "| epoch   1 |    93/ 1317 batches | loss  0.97 |\n",
      "| epoch   1 |    94/ 1317 batches | loss  0.86 |\n",
      "| epoch   1 |    95/ 1317 batches | loss  0.65 |\n",
      "| epoch   1 |    96/ 1317 batches | loss  0.98 |\n",
      "| epoch   1 |    97/ 1317 batches | loss  0.84 |\n",
      "| epoch   1 |    98/ 1317 batches | loss  0.90 |\n",
      "| epoch   1 |    99/ 1317 batches | loss  1.03 |\n",
      "| epoch   1 |   100/ 1317 batches | loss  0.66 |\n",
      "| epoch   1 |   101/ 1317 batches | loss  0.94 |\n",
      "| epoch   1 |   102/ 1317 batches | loss  1.29 |\n",
      "| epoch   1 |   103/ 1317 batches | loss  0.87 |\n",
      "| epoch   1 |   104/ 1317 batches | loss  0.75 |\n",
      "| epoch   1 |   105/ 1317 batches | loss  1.30 |\n",
      "| epoch   1 |   106/ 1317 batches | loss  1.02 |\n",
      "| epoch   1 |   107/ 1317 batches | loss  0.73 |\n",
      "| epoch   1 |   108/ 1317 batches | loss  0.94 |\n",
      "| epoch   1 |   109/ 1317 batches | loss  0.82 |\n",
      "| epoch   1 |   110/ 1317 batches | loss  0.87 |\n",
      "| epoch   1 |   111/ 1317 batches | loss  0.97 |\n",
      "| epoch   1 |   112/ 1317 batches | loss  0.85 |\n",
      "| epoch   1 |   113/ 1317 batches | loss  0.82 |\n",
      "| epoch   1 |   114/ 1317 batches | loss  0.84 |\n",
      "| epoch   1 |   115/ 1317 batches | loss  0.70 |\n",
      "| epoch   1 |   116/ 1317 batches | loss  1.05 |\n",
      "| epoch   1 |   117/ 1317 batches | loss  0.67 |\n",
      "| epoch   1 |   118/ 1317 batches | loss  0.72 |\n",
      "| epoch   1 |   119/ 1317 batches | loss  0.74 |\n",
      "| epoch   1 |   120/ 1317 batches | loss  0.74 |\n",
      "| epoch   1 |   121/ 1317 batches | loss  0.65 |\n",
      "| epoch   1 |   122/ 1317 batches | loss  0.75 |\n",
      "| epoch   1 |   123/ 1317 batches | loss  0.83 |\n",
      "| epoch   1 |   124/ 1317 batches | loss  0.74 |\n",
      "| epoch   1 |   125/ 1317 batches | loss  1.00 |\n",
      "| epoch   1 |   126/ 1317 batches | loss  0.75 |\n",
      "| epoch   1 |   127/ 1317 batches | loss  0.66 |\n",
      "| epoch   1 |   128/ 1317 batches | loss  0.61 |\n",
      "| epoch   1 |   129/ 1317 batches | loss  0.73 |\n",
      "| epoch   1 |   130/ 1317 batches | loss  0.85 |\n",
      "| epoch   1 |   131/ 1317 batches | loss  0.75 |\n",
      "| epoch   1 |   132/ 1317 batches | loss  0.68 |\n",
      "| epoch   1 |   133/ 1317 batches | loss  0.77 |\n",
      "| epoch   1 |   134/ 1317 batches | loss  0.81 |\n",
      "| epoch   1 |   135/ 1317 batches | loss  0.66 |\n",
      "| epoch   1 |   136/ 1317 batches | loss  0.65 |\n",
      "| epoch   1 |   137/ 1317 batches | loss  0.69 |\n",
      "| epoch   1 |   138/ 1317 batches | loss  0.77 |\n",
      "| epoch   1 |   139/ 1317 batches | loss  0.65 |\n",
      "| epoch   1 |   140/ 1317 batches | loss  0.91 |\n",
      "| epoch   1 |   141/ 1317 batches | loss  0.82 |\n",
      "| epoch   1 |   142/ 1317 batches | loss  0.71 |\n",
      "| epoch   1 |   143/ 1317 batches | loss  0.73 |\n",
      "| epoch   1 |   144/ 1317 batches | loss  0.87 |\n",
      "| epoch   1 |   145/ 1317 batches | loss  0.69 |\n",
      "| epoch   1 |   146/ 1317 batches | loss  0.71 |\n",
      "| epoch   1 |   147/ 1317 batches | loss  0.59 |\n",
      "| epoch   1 |   148/ 1317 batches | loss  0.67 |\n",
      "| epoch   1 |   149/ 1317 batches | loss  0.81 |\n",
      "| epoch   1 |   150/ 1317 batches | loss  1.09 |\n",
      "| epoch   1 |   151/ 1317 batches | loss  0.76 |\n",
      "| epoch   1 |   152/ 1317 batches | loss  0.98 |\n",
      "| epoch   1 |   153/ 1317 batches | loss  0.63 |\n",
      "| epoch   1 |   154/ 1317 batches | loss  0.82 |\n",
      "| epoch   1 |   155/ 1317 batches | loss  0.90 |\n",
      "| epoch   1 |   156/ 1317 batches | loss  0.69 |\n",
      "| epoch   1 |   157/ 1317 batches | loss  0.87 |\n",
      "| epoch   1 |   158/ 1317 batches | loss  0.66 |\n",
      "| epoch   1 |   159/ 1317 batches | loss  0.72 |\n",
      "| epoch   1 |   160/ 1317 batches | loss  0.92 |\n",
      "| epoch   1 |   161/ 1317 batches | loss  0.64 |\n",
      "| epoch   1 |   162/ 1317 batches | loss  0.84 |\n",
      "| epoch   1 |   163/ 1317 batches | loss  1.17 |\n",
      "| epoch   1 |   164/ 1317 batches | loss  0.80 |\n",
      "| epoch   1 |   165/ 1317 batches | loss  0.91 |\n",
      "| epoch   1 |   166/ 1317 batches | loss  0.59 |\n",
      "| epoch   1 |   167/ 1317 batches | loss  0.81 |\n",
      "| epoch   1 |   168/ 1317 batches | loss  0.89 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   169/ 1317 batches | loss  0.57 |\n",
      "| epoch   1 |   170/ 1317 batches | loss  0.72 |\n",
      "| epoch   1 |   171/ 1317 batches | loss  0.63 |\n",
      "| epoch   1 |   172/ 1317 batches | loss  0.88 |\n",
      "| epoch   1 |   173/ 1317 batches | loss  0.86 |\n",
      "| epoch   1 |   174/ 1317 batches | loss  0.74 |\n",
      "| epoch   1 |   175/ 1317 batches | loss  0.66 |\n",
      "| epoch   1 |   176/ 1317 batches | loss  0.78 |\n",
      "| epoch   1 |   177/ 1317 batches | loss  0.95 |\n",
      "| epoch   1 |   178/ 1317 batches | loss  0.77 |\n",
      "| epoch   1 |   179/ 1317 batches | loss  0.79 |\n",
      "| epoch   1 |   180/ 1317 batches | loss  0.73 |\n",
      "| epoch   1 |   181/ 1317 batches | loss  0.82 |\n",
      "| epoch   1 |   182/ 1317 batches | loss  0.86 |\n",
      "| epoch   1 |   183/ 1317 batches | loss  1.28 |\n",
      "| epoch   1 |   184/ 1317 batches | loss  0.90 |\n",
      "| epoch   1 |   185/ 1317 batches | loss  0.65 |\n",
      "| epoch   1 |   186/ 1317 batches | loss  0.88 |\n",
      "| epoch   1 |   187/ 1317 batches | loss  1.02 |\n",
      "| epoch   1 |   188/ 1317 batches | loss  0.85 |\n",
      "| epoch   1 |   189/ 1317 batches | loss  0.76 |\n",
      "| epoch   1 |   190/ 1317 batches | loss  0.79 |\n",
      "| epoch   1 |   191/ 1317 batches | loss  0.68 |\n",
      "| epoch   1 |   192/ 1317 batches | loss  1.16 |\n",
      "| epoch   1 |   193/ 1317 batches | loss  0.81 |\n",
      "| epoch   1 |   194/ 1317 batches | loss  1.21 |\n",
      "| epoch   1 |   195/ 1317 batches | loss  0.71 |\n",
      "| epoch   1 |   196/ 1317 batches | loss  0.81 |\n",
      "| epoch   1 |   197/ 1317 batches | loss  0.70 |\n",
      "| epoch   1 |   198/ 1317 batches | loss  0.65 |\n",
      "| epoch   1 |   199/ 1317 batches | loss  0.62 |\n",
      "| epoch   1 |   200/ 1317 batches | loss  0.64 |\n",
      "| epoch   1 |   201/ 1317 batches | loss  0.78 |\n",
      "| epoch   1 |   202/ 1317 batches | loss  0.73 |\n",
      "| epoch   1 |   203/ 1317 batches | loss  0.74 |\n",
      "| epoch   1 |   204/ 1317 batches | loss  0.72 |\n",
      "| epoch   1 |   205/ 1317 batches | loss  0.93 |\n",
      "| epoch   1 |   206/ 1317 batches | loss  0.72 |\n",
      "| epoch   1 |   207/ 1317 batches | loss  0.77 |\n",
      "| epoch   1 |   208/ 1317 batches | loss  0.97 |\n",
      "| epoch   1 |   209/ 1317 batches | loss  0.82 |\n",
      "| epoch   1 |   210/ 1317 batches | loss  0.91 |\n",
      "| epoch   1 |   211/ 1317 batches | loss  0.76 |\n",
      "| epoch   1 |   212/ 1317 batches | loss  1.18 |\n",
      "| epoch   1 |   213/ 1317 batches | loss  1.20 |\n",
      "| epoch   1 |   214/ 1317 batches | loss  1.32 |\n",
      "| epoch   1 |   215/ 1317 batches | loss  1.07 |\n",
      "| epoch   1 |   216/ 1317 batches | loss  0.67 |\n",
      "| epoch   1 |   217/ 1317 batches | loss  0.93 |\n",
      "| epoch   1 |   218/ 1317 batches | loss  0.62 |\n",
      "| epoch   1 |   219/ 1317 batches | loss  0.67 |\n",
      "| epoch   1 |   220/ 1317 batches | loss  0.62 |\n",
      "| epoch   1 |   221/ 1317 batches | loss  1.11 |\n",
      "| epoch   1 |   222/ 1317 batches | loss  0.66 |\n",
      "| epoch   1 |   223/ 1317 batches | loss  0.76 |\n",
      "| epoch   1 |   224/ 1317 batches | loss  0.66 |\n",
      "| epoch   1 |   225/ 1317 batches | loss  0.66 |\n",
      "| epoch   1 |   226/ 1317 batches | loss  0.77 |\n",
      "| epoch   1 |   227/ 1317 batches | loss  0.72 |\n",
      "| epoch   1 |   228/ 1317 batches | loss  0.60 |\n",
      "| epoch   1 |   229/ 1317 batches | loss  0.98 |\n",
      "| epoch   1 |   230/ 1317 batches | loss  1.05 |\n",
      "| epoch   1 |   231/ 1317 batches | loss  0.81 |\n",
      "| epoch   1 |   232/ 1317 batches | loss  1.30 |\n",
      "| epoch   1 |   233/ 1317 batches | loss  0.76 |\n",
      "| epoch   1 |   234/ 1317 batches | loss  0.81 |\n",
      "| epoch   1 |   235/ 1317 batches | loss  0.97 |\n",
      "| epoch   1 |   236/ 1317 batches | loss  0.98 |\n",
      "| epoch   1 |   237/ 1317 batches | loss  0.62 |\n",
      "| epoch   1 |   238/ 1317 batches | loss  1.02 |\n",
      "| epoch   1 |   239/ 1317 batches | loss  1.18 |\n",
      "| epoch   1 |   240/ 1317 batches | loss  0.67 |\n",
      "| epoch   1 |   241/ 1317 batches | loss  1.05 |\n",
      "| epoch   1 |   242/ 1317 batches | loss  0.90 |\n",
      "| epoch   1 |   243/ 1317 batches | loss  0.75 |\n",
      "| epoch   1 |   244/ 1317 batches | loss  1.04 |\n",
      "| epoch   1 |   245/ 1317 batches | loss  0.67 |\n",
      "| epoch   1 |   246/ 1317 batches | loss  1.00 |\n",
      "| epoch   1 |   247/ 1317 batches | loss  0.68 |\n",
      "| epoch   1 |   248/ 1317 batches | loss  0.88 |\n",
      "| epoch   1 |   249/ 1317 batches | loss  0.75 |\n",
      "| epoch   1 |   250/ 1317 batches | loss  0.89 |\n",
      "| epoch   1 |   251/ 1317 batches | loss  0.84 |\n",
      "| epoch   1 |   252/ 1317 batches | loss  0.62 |\n",
      "| epoch   1 |   253/ 1317 batches | loss  0.76 |\n",
      "| epoch   1 |   254/ 1317 batches | loss  0.71 |\n",
      "| epoch   1 |   255/ 1317 batches | loss  0.88 |\n",
      "| epoch   1 |   256/ 1317 batches | loss  0.70 |\n",
      "| epoch   1 |   257/ 1317 batches | loss  0.97 |\n",
      "| epoch   1 |   258/ 1317 batches | loss  0.71 |\n",
      "| epoch   1 |   259/ 1317 batches | loss  0.79 |\n",
      "| epoch   1 |   260/ 1317 batches | loss  0.97 |\n",
      "| epoch   1 |   261/ 1317 batches | loss  0.87 |\n",
      "| epoch   1 |   262/ 1317 batches | loss  1.66 |\n",
      "| epoch   1 |   263/ 1317 batches | loss  1.30 |\n",
      "| epoch   1 |   264/ 1317 batches | loss  0.98 |\n",
      "| epoch   1 |   265/ 1317 batches | loss  2.21 |\n",
      "| epoch   1 |   266/ 1317 batches | loss  0.91 |\n",
      "| epoch   1 |   267/ 1317 batches | loss  0.99 |\n",
      "| epoch   1 |   268/ 1317 batches | loss  1.30 |\n",
      "| epoch   1 |   269/ 1317 batches | loss  2.80 |\n",
      "| epoch   1 |   270/ 1317 batches | loss  0.70 |\n",
      "| epoch   1 |   271/ 1317 batches | loss  1.77 |\n",
      "| epoch   1 |   272/ 1317 batches | loss  2.21 |\n",
      "| epoch   1 |   273/ 1317 batches | loss  1.24 |\n",
      "| epoch   1 |   274/ 1317 batches | loss  0.96 |\n",
      "| epoch   1 |   275/ 1317 batches | loss  1.26 |\n",
      "| epoch   1 |   276/ 1317 batches | loss  0.63 |\n",
      "| epoch   1 |   277/ 1317 batches | loss  0.61 |\n",
      "| epoch   1 |   278/ 1317 batches | loss  0.88 |\n",
      "| epoch   1 |   279/ 1317 batches | loss  0.79 |\n",
      "| epoch   1 |   280/ 1317 batches | loss  0.82 |\n",
      "| epoch   1 |   281/ 1317 batches | loss  0.85 |\n",
      "| epoch   1 |   282/ 1317 batches | loss  0.72 |\n",
      "| epoch   1 |   283/ 1317 batches | loss  0.62 |\n",
      "| epoch   1 |   284/ 1317 batches | loss  0.92 |\n",
      "| epoch   1 |   285/ 1317 batches | loss  0.95 |\n",
      "| epoch   1 |   286/ 1317 batches | loss  0.63 |\n",
      "| epoch   1 |   287/ 1317 batches | loss  2.00 |\n",
      "| epoch   1 |   288/ 1317 batches | loss  0.90 |\n",
      "| epoch   1 |   289/ 1317 batches | loss  1.00 |\n",
      "| epoch   1 |   290/ 1317 batches | loss  1.05 |\n",
      "| epoch   1 |   291/ 1317 batches | loss  0.70 |\n",
      "| epoch   1 |   292/ 1317 batches | loss  0.79 |\n",
      "| epoch   1 |   293/ 1317 batches | loss  0.48 |\n",
      "| epoch   1 |   294/ 1317 batches | loss  0.70 |\n",
      "| epoch   1 |   295/ 1317 batches | loss  0.77 |\n",
      "| epoch   1 |   296/ 1317 batches | loss  0.62 |\n",
      "| epoch   1 |   297/ 1317 batches | loss  0.81 |\n",
      "| epoch   1 |   298/ 1317 batches | loss  0.76 |\n",
      "| epoch   1 |   299/ 1317 batches | loss  1.11 |\n",
      "| epoch   1 |   300/ 1317 batches | loss  0.88 |\n",
      "| epoch   1 |   301/ 1317 batches | loss  0.69 |\n",
      "| epoch   1 |   302/ 1317 batches | loss  1.08 |\n",
      "| epoch   1 |   303/ 1317 batches | loss  1.06 |\n",
      "| epoch   1 |   304/ 1317 batches | loss  0.70 |\n",
      "| epoch   1 |   305/ 1317 batches | loss  1.13 |\n",
      "| epoch   1 |   306/ 1317 batches | loss  1.19 |\n",
      "| epoch   1 |   307/ 1317 batches | loss  0.71 |\n",
      "| epoch   1 |   308/ 1317 batches | loss  0.92 |\n",
      "| epoch   1 |   309/ 1317 batches | loss  0.94 |\n",
      "| epoch   1 |   310/ 1317 batches | loss  0.91 |\n",
      "| epoch   1 |   311/ 1317 batches | loss  0.89 |\n",
      "| epoch   1 |   312/ 1317 batches | loss  0.73 |\n",
      "| epoch   1 |   313/ 1317 batches | loss  0.82 |\n",
      "| epoch   1 |   314/ 1317 batches | loss  0.85 |\n",
      "| epoch   1 |   315/ 1317 batches | loss  0.76 |\n",
      "| epoch   1 |   316/ 1317 batches | loss  0.72 |\n",
      "| epoch   1 |   317/ 1317 batches | loss  0.75 |\n",
      "| epoch   1 |   318/ 1317 batches | loss  0.86 |\n",
      "| epoch   1 |   319/ 1317 batches | loss  0.97 |\n",
      "| epoch   1 |   320/ 1317 batches | loss  0.72 |\n",
      "| epoch   1 |   321/ 1317 batches | loss  0.73 |\n",
      "| epoch   1 |   322/ 1317 batches | loss  0.67 |\n",
      "| epoch   1 |   323/ 1317 batches | loss  0.83 |\n",
      "| epoch   1 |   324/ 1317 batches | loss  0.93 |\n",
      "| epoch   1 |   325/ 1317 batches | loss  1.07 |\n",
      "| epoch   1 |   326/ 1317 batches | loss  0.81 |\n",
      "| epoch   1 |   327/ 1317 batches | loss  1.53 |\n",
      "| epoch   1 |   328/ 1317 batches | loss  0.85 |\n",
      "| epoch   1 |   329/ 1317 batches | loss  0.71 |\n",
      "| epoch   1 |   330/ 1317 batches | loss  0.88 |\n",
      "| epoch   1 |   331/ 1317 batches | loss  1.40 |\n",
      "| epoch   1 |   332/ 1317 batches | loss  0.67 |\n",
      "| epoch   1 |   333/ 1317 batches | loss  2.43 |\n",
      "| epoch   1 |   334/ 1317 batches | loss  1.20 |\n",
      "| epoch   1 |   335/ 1317 batches | loss  1.13 |\n",
      "| epoch   1 |   336/ 1317 batches | loss  1.05 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   337/ 1317 batches | loss  0.74 |\n",
      "| epoch   1 |   338/ 1317 batches | loss  1.00 |\n",
      "| epoch   1 |   339/ 1317 batches | loss  0.62 |\n",
      "| epoch   1 |   340/ 1317 batches | loss  0.71 |\n",
      "| epoch   1 |   341/ 1317 batches | loss  1.04 |\n",
      "| epoch   1 |   342/ 1317 batches | loss  0.65 |\n",
      "| epoch   1 |   343/ 1317 batches | loss  0.74 |\n",
      "| epoch   1 |   344/ 1317 batches | loss  0.80 |\n",
      "| epoch   1 |   345/ 1317 batches | loss  0.73 |\n",
      "| epoch   1 |   346/ 1317 batches | loss  0.86 |\n",
      "| epoch   1 |   347/ 1317 batches | loss  0.73 |\n",
      "| epoch   1 |   348/ 1317 batches | loss  0.90 |\n",
      "| epoch   1 |   349/ 1317 batches | loss  0.67 |\n",
      "| epoch   1 |   350/ 1317 batches | loss  0.65 |\n",
      "| epoch   1 |   351/ 1317 batches | loss  0.98 |\n",
      "| epoch   1 |   352/ 1317 batches | loss  0.81 |\n",
      "| epoch   1 |   353/ 1317 batches | loss  0.77 |\n",
      "| epoch   1 |   354/ 1317 batches | loss  0.97 |\n",
      "| epoch   1 |   355/ 1317 batches | loss  0.51 |\n",
      "| epoch   1 |   356/ 1317 batches | loss  1.21 |\n",
      "| epoch   1 |   357/ 1317 batches | loss  0.91 |\n",
      "| epoch   1 |   358/ 1317 batches | loss  0.72 |\n",
      "| epoch   1 |   359/ 1317 batches | loss  0.67 |\n",
      "| epoch   1 |   360/ 1317 batches | loss  0.74 |\n",
      "| epoch   1 |   361/ 1317 batches | loss  0.79 |\n",
      "| epoch   1 |   362/ 1317 batches | loss  0.87 |\n",
      "| epoch   1 |   363/ 1317 batches | loss  0.60 |\n",
      "| epoch   1 |   364/ 1317 batches | loss  0.72 |\n",
      "| epoch   1 |   365/ 1317 batches | loss  0.91 |\n",
      "| epoch   1 |   366/ 1317 batches | loss  0.73 |\n",
      "| epoch   1 |   367/ 1317 batches | loss  1.28 |\n",
      "| epoch   1 |   368/ 1317 batches | loss  0.89 |\n",
      "| epoch   1 |   369/ 1317 batches | loss  0.97 |\n",
      "| epoch   1 |   370/ 1317 batches | loss  0.64 |\n",
      "| epoch   1 |   371/ 1317 batches | loss  0.58 |\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 5.94 GiB total capacity; 4.82 GiB already allocated; 11.31 MiB free; 240.13 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0d57f77ded78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         ctx = model.forward(input_ids=ctx,\n\u001b[1;32m     10\u001b[0m                            \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_ctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                             position_ids=pos_token)[0]\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mctx\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m                             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m                             inputs_embeds=inputs_embeds)\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    738\u001b[0m                                        \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                                        \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                                        encoder_attention_mask=encoder_extended_attention_mask)\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mself_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# Mask heads if we want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    805\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    806\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 5.94 GiB total capacity; 4.82 GiB already allocated; 11.31 MiB free; 240.13 MiB cached)"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    for ctx,pos_token,pos_ctx,label in train_loader:\n",
    "        i+=1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #pre-trained BERT\n",
    "        ctx = model.forward(input_ids=ctx,\n",
    "                           token_type_ids=pos_ctx,\n",
    "                            position_ids=pos_token)[0]\n",
    "        \n",
    "        loss = loss_func( ctx , label )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "              'loss {:5.2f} |'.format(\n",
    "                epoch+1, i, n,loss.item()))\n",
    "    print('-' * 35)\n",
    "    print(\"Epoch \",epoch,\"\\t\",round(total_loss / n,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ctx,pos_token,pos_ctx,label in train_loader:\n",
    "    print(code2string(ctx[0],dict_token))\n",
    "    print(pos_ctx[0])\n",
    "    print(label[0].item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.LongTensor([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50 % 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
